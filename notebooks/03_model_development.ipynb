{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03_model_development\n",
        "\n",
        "## Carbon Sequestration Prediction Modeling\n",
        "\n",
        "**Objectives:**\n",
        "- Train carbon sequestration prediction models\n",
        "- Compare multiple algorithms (Random Forest, XGBoost, Neural Networks)\n",
        "- Hyperparameter tuning with cross-validation\n",
        "- Comprehensive model evaluation\n",
        "- Feature importance analysis\n",
        "\n",
        "**Target Variable:** Carbon Stock (kg/m¬≤)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Dependencies and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# Advanced Models\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "\n",
        "# Visualization\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import joblib\n",
        "\n",
        "# Setup\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ All dependencies imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading and Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load processed data from EDA\n",
        "try:\n",
        "    biomass_df = pd.read_csv('outputs/biomass_data_with_insights.csv')\n",
        "    json_df = pd.read_csv('outputs/json_data_with_insights.csv')\n",
        "    print(\"‚úÖ Successfully loaded processed data from EDA\")\n",
        "except FileNotFoundError:\n",
        "    # Fallback to original data\n",
        "    try:\n",
        "        biomass_df = pd.read_csv('data/processed_biomass_csv.csv')\n",
        "        json_df = pd.read_csv('data/processed_biomass_json.csv')\n",
        "        print(\"‚úÖ Loaded processed data from data directory\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ùå No processed data found. Please run previous notebooks first.\")\n",
        "        raise\n",
        "\n",
        "# Combine datasets for larger training set\n",
        "def prepare_combined_dataset(csv_df, json_df):\n",
        "    \"\"\"Combine and prepare datasets for modeling.\"\"\"\n",
        "    \n",
        "    # Standardize column names\n",
        "    if 'biomass_value' in csv_df.columns:\n",
        "        csv_df = csv_df.rename(columns={'biomass_value': 'biomass'})\n",
        "    \n",
        "    # Select common columns\n",
        "    common_cols = ['biomass', 'carbon_stock', 'latitude', 'longitude']\n",
        "    \n",
        "    # Add vegetation type if available\n",
        "    if 'vegetation_type' in csv_df.columns:\n",
        "        common_cols.append('vegetation_type')\n",
        "    \n",
        "    if 'quality_flag' in json_df.columns:\n",
        "        common_cols.append('quality_flag')\n",
        "    \n",
        "    # Combine datasets\n",
        "    combined_df = pd.concat([\n",
        "        csv_df[common_cols],\n",
        "        json_df[common_cols]\n",
        "    ], ignore_index=True)\n",
        "    \n",
        "    return combined_df\n",
        "\n",
        "# Prepare combined dataset\n",
        "modeling_df = prepare_combined_dataset(biomass_df, json_df)\n",
        "print(f\"üìä Combined dataset shape: {modeling_df.shape}\")\n",
        "print(f\"üìà Target variable: carbon_stock\")\n",
        "print(f\"üéØ Target distribution:\")\n",
        "print(modeling_df['carbon_stock'].describe())\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nFirst 5 rows of combined data:\")\n",
        "display(modeling_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CarbonFeatureEngineer:\n",
        "    \"\"\"Feature engineering for carbon sequestration prediction.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.label_encoders = {}\n",
        "        self.scaler = StandardScaler()\n",
        "    \n",
        "    def create_spatial_features(self, df):\n",
        "        \"\"\"Create spatial and geographic features.\"\"\"\n",
        "        df_eng = df.copy()\n",
        "        \n",
        "        # Spatial clustering features\n",
        "        df_eng['lat_bin'] = pd.cut(df_eng['latitude'], bins=5, labels=False)\n",
        "        df_eng['lon_bin'] = pd.cut(df_eng['longitude'], bins=5, labels=False)\n",
        "        \n",
        "        # Distance from center\n",
        "        center_lat = df_eng['latitude'].mean()\n",
        "        center_lon = df_eng['longitude'].mean()\n",
        "        df_eng['distance_from_center'] = np.sqrt(\n",
        "            (df_eng['latitude'] - center_lat)**2 + \n",
        "            (df_eng['longitude'] - center_lon)**2\n",
        "        )\n",
        "        \n",
        "        # Spatial interaction terms\n",
        "        df_eng['lat_lon_interaction'] = df_eng['latitude'] * df_eng['longitude']\n",
        "        df_eng['spatial_cluster'] = (df_eng['lat_bin'] * 10 + df_eng['lon_bin']).astype(int)\n",
        "        \n",
        "        return df_eng\n",
        "    \n",
        "    def create_biomass_features(self, df):\n",
        "        \"\"\"Create biomass-derived features.\"\"\"\n",
        "        df_eng = df.copy()\n",
        "        \n",
        "        # Biomass transformations\n",
        "        df_eng['log_biomass'] = np.log1p(df_eng['biomass'])\n",
        "        df_eng['sqrt_biomass'] = np.sqrt(df_eng['biomass'])\n",
        "        df_eng['biomass_squared'] = df_eng['biomass'] ** 2\n",
        "        \n",
        "        # Biomass categories\n",
        "        df_eng['biomass_category'] = pd.cut(\n",
        "            df_eng['biomass'], \n",
        "            bins=[0, 1, 2, 3, 5, np.inf], \n",
        "            labels=['Very_Low', 'Low', 'Medium', 'High', 'Very_High']\n",
        "        )\n",
        "        \n",
        "        return df_eng\n",
        "    \n",
        "    def encode_categorical_features(self, df):\n",
        "        \"\"\"Encode categorical variables.\"\"\"\n",
        "        df_eng = df.copy()\n",
        "        \n",
        "        categorical_columns = ['biomass_category']\n",
        "        if 'vegetation_type' in df_eng.columns:\n",
        "            categorical_columns.append('vegetation_type')\n",
        "        if 'quality_flag' in df_eng.columns:\n",
        "            categorical_columns.append('quality_flag')\n",
        "        \n",
        "        for col in categorical_columns:\n",
        "            if col in df_eng.columns:\n",
        "                le = LabelEncoder()\n",
        "                df_eng[col + '_encoded'] = le.fit_transform(df_eng[col].astype(str))\n",
        "                self.label_encoders[col] = le\n",
        "        \n",
        "        return df_eng\n",
        "    \n",
        "    def create_interaction_features(self, df):\n",
        "        \"\"\"Create interaction features between variables.\"\"\"\n",
        "        df_eng = df.copy()\n",
        "        \n",
        "        # Biomass-spatial interactions\n",
        "        df_eng['biomass_latitude'] = df_eng['biomass'] * df_eng['latitude']\n",
        "        df_eng['biomass_longitude'] = df_eng['biomass'] * df_eng['longitude']\n",
        "        \n",
        "        # Polynomial features for key variables\n",
        "        poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
        "        interaction_features = poly.fit_transform(df_eng[['biomass', 'latitude', 'longitude']])\n",
        "        interaction_df = pd.DataFrame(\n",
        "            interaction_features, \n",
        "            columns=poly.get_feature_names_out(['biomass', 'latitude', 'longitude'])\n",
        "        )\n",
        "        \n",
        "        df_eng = pd.concat([df_eng, interaction_df], axis=1)\n",
        "        \n",
        "        return df_eng\n",
        "    \n",
        "    def prepare_features(self, df, target_col='carbon_stock'):\n",
        "        \"\"\"Complete feature engineering pipeline.\"\"\"\n",
        "        print(\"üîß Starting feature engineering...\")\n",
        "        \n",
        "        # Apply all feature engineering steps\n",
        "        df_eng = self.create_spatial_features(df)\n",
        "        df_eng = self.create_biomass_features(df_eng)\n",
        "        df_eng = self.encode_categorical_features(df_eng)\n",
        "        df_eng = self.create_interaction_features(df_eng)\n",
        "        \n",
        "        # Remove original categorical columns and target\n",
        "        columns_to_drop = ['biomass_category']\n",
        "        if 'vegetation_type' in df_eng.columns:\n",
        "            columns_to_drop.append('vegetation_type')\n",
        "        if 'quality_flag' in df_eng.columns:\n",
        "            columns_to_drop.append('quality_flag')\n",
        "        \n",
        "        X = df_eng.drop(columns=[target_col] + columns_to_drop, errors='ignore')\n",
        "        y = df_eng[target_col]\n",
        "        \n",
        "        # Remove any remaining non-numeric columns\n",
        "        X = X.select_dtypes(include=[np.number])\n",
        "        \n",
        "        print(f\"‚úÖ Feature engineering complete. Final feature count: {X.shape[1]}\")\n",
        "        print(f\"üìä Features: {list(X.columns)}\")\n",
        "        \n",
        "        return X, y\n",
        "\n",
        "# Initialize feature engineer\n",
        "feature_engineer = CarbonFeatureEngineer()\n",
        "\n",
        "# Prepare features and target\n",
        "X, y = feature_engineer.prepare_features(modeling_df)\n",
        "\n",
        "# Display feature information\n",
        "print(f\"\\nüìà Feature matrix shape: {X.shape}\")\n",
        "print(f\"üéØ Target vector shape: {y.shape}\")\n",
        "print(f\"\\nüìã First 5 features:\")\n",
        "display(X.head())\n",
        "\n",
        "# Feature correlation with target\n",
        "feature_correlations = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'correlation_with_carbon': [X[col].corr(y) for col in X.columns]\n",
        "}).sort_values('correlation_with_carbon', key=abs, ascending=False)\n",
        "\n",
        "print(\"\\nüîó Top 10 features by absolute correlation with carbon stock:\")\n",
        "display(feature_correlations.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train-Test Split and Data Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "print(\"üìä Dataset Split Summary:\")\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"Number of features: {X_train.shape[1]}\")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"\\n‚úÖ Feature scaling completed\")\n",
        "print(f\"Training set mean after scaling: {X_train_scaled.mean():.4f}\")\n",
        "print(f\"Training set std after scaling: {X_train_scaled.std():.4f}\")\n",
        "\n",
        "# Convert back to DataFrames for easier handling\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Training - Multiple Algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CarbonModelTrainer:\n",
        "    \"\"\"Train and compare multiple models for carbon prediction.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.results = {}\n",
        "        self.best_model = None\n",
        "        self.best_score = -np.inf\n",
        "    \n",
        "    def initialize_models(self):\n",
        "        \"\"\"Initialize multiple regression models.\"\"\"\n",
        "        \n",
        "        self.models = {\n",
        "            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "            'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42, verbosity=0),\n",
        "            'LightGBM': lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1),\n",
        "            'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
        "            'Support Vector Machine': SVR(kernel='rbf', C=1.0),\n",
        "            'Neural Network': MLPRegressor(hidden_layer_sizes=(100, 50), random_state=42, max_iter=1000),\n",
        "            'Ridge Regression': Ridge(alpha=1.0),\n",
        "            'Lasso Regression': Lasso(alpha=1.0)\n",
        "        }\n",
        "        \n",
        "        print(\"‚úÖ Initialized 8 different models for comparison\")\n",
        "    \n",
        "    def train_models(self, X_train, y_train, X_test, y_test, cv_folds=5):\n",
        "        \"\"\"Train all models and evaluate performance.\"\"\"\n",
        "        \n",
        "        self.results = {}\n",
        "        \n",
        "        for name, model in self.models.items():\n",
        "            print(f\"\\nüöÄ Training {name}...\")\n",
        "            \n",
        "            # Train model\n",
        "            model.fit(X_train, y_train)\n",
        "            \n",
        "            # Predictions\n",
        "            y_train_pred = model.predict(X_train)\n",
        "            y_test_pred = model.predict(X_test)\n",
        "            \n",
        "            # Calculate metrics\n",
        "            train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "            test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "            train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "            test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "            train_r2 = r2_score(y_train, y_train_pred)\n",
        "            test_r2 = r2_score(y_test, y_test_pred)\n",
        "            \n",
        "            # Cross-validation\n",
        "            cv_scores = cross_val_score(model, X_train, y_train, cv=cv_folds, scoring='r2')\n",
        "            \n",
        "            # Store results\n",
        "            self.results[name] = {\n",
        "                'model': model,\n",
        "                'train_rmse': train_rmse,\n",
        "                'test_rmse': test_rmse,\n",
        "                'train_mae': train_mae,\n",
        "                'test_mae': test_mae,\n",
        "                'train_r2': train_r2,\n",
        "                'test_r2': test_r2,\n",
        "                'cv_mean': cv_scores.mean(),\n",
        "                'cv_std': cv_scores.std(),\n",
        "                'predictions': y_test_pred\n",
        "            }\n",
        "            \n",
        "            print(f\"   ‚úÖ {name} trained:\")\n",
        "            print(f\"      Test R¬≤: {test_r2:.4f}, Test RMSE: {test_rmse:.4f}\")\n",
        "            \n",
        "            # Update best model\n",
        "            if test_r2 > self.best_score:\n",
        "                self.best_score = test_r2\n",
        "                self.best_model = name\n",
        "    \n",
        "    def compare_models(self):\n",
        "        \"\"\"Compare model performance and create visualizations.\"\"\"\n",
        "        \n",
        "        # Create results DataFrame\n",
        "        comparison_df = pd.DataFrame({\n",
        "            'Model': list(self.results.keys()),\n",
        "            'Test_R2': [self.results[name]['test_r2'] for name in self.results],\n",
        "            'Test_RMSE': [self.results[name]['test_rmse'] for name in self.results],\n",
        "            'Test_MAE': [self.results[name]['test_mae'] for name in self.results],\n",
        "            'CV_Mean_R2': [self.results[name]['cv_mean'] for name in self.results],\n",
        "            'CV_Std_R2': [self.results[name]['cv_std'] for name in self.results]\n",
        "        }).sort_values('Test_R2', ascending=False)\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"üìä MODEL COMPARISON RESULTS\")\n",
        "        print(\"=\"*80)\n",
        "        display(comparison_df.round(4))\n",
        "        \n",
        "        print(f\"\\nüèÜ BEST MODEL: {self.best_model} (Test R¬≤: {self.best_score:.4f})\")\n",
        "        \n",
        "        # Create visualizations\n",
        "        self._create_comparison_plots(comparison_df)\n",
        "        \n",
        "        return comparison_df\n",
        "    \n",
        "    def _create_comparison_plots(self, comparison_df):\n",
        "        \"\"\"Create comparison plots for model performance.\"\"\"\n",
        "        \n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=(\n",
        "                'Model R¬≤ Scores Comparison',\n",
        "                'Model RMSE Comparison',\n",
        "                'Cross-Validation Performance',\n",
        "                'Prediction vs Actual (Best Model)'\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        # 1. R¬≤ comparison\n",
        "        fig.add_trace(\n",
        "            go.Bar(x=comparison_df['Model'], y=comparison_df['Test_R2'],\n",
        "                  name='Test R¬≤', marker_color='lightblue'),\n",
        "            row=1, col=1\n",
        "        )\n",
        "        \n",
        "        # 2. RMSE comparison\n",
        "        fig.add_trace(\n",
        "            go.Bar(x=comparison_df['Model'], y=comparison_df['Test_RMSE'],\n",
        "                  name='Test RMSE', marker_color='lightcoral'),\n",
        "            row=1, col=2\n",
        "        )\n",
        "        \n",
        "        # 3. Cross-validation performance\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=comparison_df['Model'], y=comparison_df['CV_Mean_R2'],\n",
        "                      mode='markers+lines', name='CV Mean R¬≤',\n",
        "                      error_y=dict(type='data', array=comparison_df['CV_Std_R2'], visible=True)),\n",
        "            row=2, col=1\n",
        "        )\n",
        "        \n",
        "        # 4. Prediction vs Actual for best model\n",
        "        best_model_name = self.best_model\n",
        "        best_predictions = self.results[best_model_name]['predictions']\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=y_test, y=best_predictions, mode='markers',\n",
        "                      name=f'{best_model_name} Predictions',\n",
        "                      marker=dict(color='green', opacity=0.6)),\n",
        "            row=2, col=2\n",
        "        )\n",
        "        \n",
        "        # Add perfect prediction line\n",
        "        min_val = min(y_test.min(), best_predictions.min())\n",
        "        max_val = max(y_test.max(), best_predictions.max())\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=[min_val, max_val], y=[min_val, max_val],\n",
        "                      mode='lines', name='Perfect Prediction',\n",
        "                      line=dict(color='red', dash='dash')),\n",
        "            row=2, col=2\n",
        "        )\n",
        "        \n",
        "        fig.update_layout(\n",
        "            title_text=\"Model Performance Comparison\",\n",
        "            height=800,\n",
        "            showlegend=True\n",
        "        )\n",
        "        \n",
        "        fig.show()\n",
        "\n",
        "# Initialize and run model trainer\n",
        "model_trainer = CarbonModelTrainer()\n",
        "model_trainer.initialize_models()\n",
        "model_trainer.train_models(X_train_scaled_df, y_train, X_test_scaled_df, y_test)\n",
        "comparison_results = model_trainer.compare_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Hyperparameter Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HyperparameterOptimizer:\n",
        "    \"\"\"Optimize hyperparameters for top performing models.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.optimized_models = {}\n",
        "        self.best_params = {}\n",
        "    \n",
        "    def optimize_random_forest(self, X_train, y_train, cv_folds=5):\n",
        "        \"\"\"Optimize Random Forest hyperparameters.\"\"\"\n",
        "        \n",
        "        param_grid = {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [10, 20, 30, None],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4],\n",
        "            'max_features': ['sqrt', 'log2']\n",
        "        }\n",
        "        \n",
        "        rf = RandomForestRegressor(random_state=42)\n",
        "        \n",
        "        # Use RandomizedSearchCV for faster optimization\n",
        "        search = RandomizedSearchCV(\n",
        "            rf, param_grid, n_iter=50, cv=cv_folds, \n",
        "            scoring='r2', random_state=42, n_jobs=-1\n",
        "        )\n",
        "        \n",
        "        print(\"üîç Optimizing Random Forest...\")\n",
        "        search.fit(X_train, y_train)\n",
        "        \n",
        "        self.optimized_models['Random Forest'] = search.best_estimator_\n",
        "        self.best_params['Random Forest'] = search.best_params_\n",
        "        \n",
        "        print(f\"‚úÖ Best Random Forest R¬≤: {search.best_score_:.4f}\")\n",
        "        print(f\"üìã Best parameters: {search.best_params_}\")\n",
        "        \n",
        "        return search.best_estimator_\n",
        "    \n",
        "    def optimize_xgboost(self, X_train, y_train, cv_folds=5):\n",
        "        \"\"\"Optimize XGBoost hyperparameters.\"\"\"\n",
        "        \n",
        "        param_grid = {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [3, 6, 9],\n",
        "            'learning_rate': [0.01, 0.1, 0.2],\n",
        "            'subsample': [0.8, 0.9, 1.0],\n",
        "            'colsample_bytree': [0.8, 0.9, 1.0]\n",
        "        }\n",
        "        \n",
        "        xgb_model = xgb.XGBRegressor(random_state=42, verbosity=0)\n",
        "        \n",
        "        search = RandomizedSearchCV(\n",
        "            xgb_model, param_grid, n_iter=50, cv=cv_folds,\n",
        "            scoring='r2', random_state=42, n_jobs=-1\n",
        "        )\n",
        "        \n",
        "        print(\"\\nüîç Optimizing XGBoost...\")\n",
        "        search.fit(X_train, y_train)\n",
        "        \n",
        "        self.optimized_models['XGBoost'] = search.best_estimator_\n",
        "        self.best_params['XGBoost'] = search.best_params_\n",
        "        \n",
        "        print(f\"‚úÖ Best XGBoost R¬≤: {search.best_score_:.4f}\")\n",
        "        print(f\"üìã Best parameters: {search.best_params_}\")\n",
        "        \n",
        "        return search.best_estimator_\n",
        "    \n",
        "    def optimize_all_models(self, X_train, y_train, top_n=3):\n",
        "        \"\"\"Optimize top N performing models.\"\"\"\n",
        "        \n",
        "        print(\"=\"*80)\n",
        "        print(\"üéØ HYPERPARAMETER OPTIMIZATION\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        # Get top models from previous results\n",
        "        top_models = []\n",
        "        if hasattr(model_trainer, 'results'):\n",
        "            sorted_models = sorted(\n",
        "                model_trainer.results.items(), \n",
        "                key=lambda x: x[1]['test_r2'], \n",
        "                reverse=True\n",
        "            )\n",
        "            top_models = [name for name, _ in sorted_models[:top_n]]\n",
        "        \n",
        "        print(f\"Optimizing top {len(top_models)} models: {top_models}\")\n",
        "        \n",
        "        # Optimize each top model\n",
        "        for model_name in top_models:\n",
        "            if model_name == 'Random Forest':\n",
        "                self.optimize_random_forest(X_train, y_train)\n",
        "            elif model_name == 'XGBoost':\n",
        "                self.optimize_xgboost(X_train, y_train)\n",
        "            elif model_name == 'LightGBM':\n",
        "                # Add LightGBM optimization if needed\n",
        "                pass\n",
        "    \n",
        "    def evaluate_optimized_models(self, X_test, y_test):\n",
        "        \"\"\"Evaluate optimized models on test set.\"\"\"\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"üìä OPTIMIZED MODELS EVALUATION\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        optimized_results = {}\n",
        "        \n",
        "        for name, model in self.optimized_models.items():\n",
        "            y_pred = model.predict(X_test)\n",
        "            \n",
        "            test_r2 = r2_score(y_test, y_pred)\n",
        "            test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "            test_mae = mean_absolute_error(y_test, y_pred)\n",
        "            \n",
        "            optimized_results[name] = {\n",
        "                'test_r2': test_r2,\n",
        "                'test_rmse': test_rmse,\n",
        "                'test_mae': test_mae,\n",
        "                'model': model\n",
        "            }\n",
        "            \n",
        "            print(f\"\\n{name} - Optimized:\")\n",
        "            print(f\"  Test R¬≤: {test_r2:.4f}\")\n",
        "            print(f\"  Test RMSE: {test_rmse:.4f}\")\n",
        "            print(f\"  Test MAE: {test_mae:.4f}\")\n",
        "            \n",
        "            # Compare with baseline\n",
        "            if hasattr(model_trainer, 'results') and name in model_trainer.results:\n",
        "                baseline_r2 = model_trainer.results[name]['test_r2']\n",
        "                improvement = test_r2 - baseline_r2\n",
        "                print(f\"  Improvement: {improvement:+.4f}\")\n",
        "        \n",
        "        return optimized_results\n",
        "\n",
        "# Run hyperparameter optimization\n",
        "optimizer = HyperparameterOptimizer()\n",
        "optimizer.optimize_all_models(X_train_scaled_df, y_train)\n",
        "optimized_results = optimizer.evaluate_optimized_models(X_test_scaled_df, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Feature Importance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeatureImportanceAnalyzer:\n",
        "    \"\"\"Analyze feature importance across different models.\"\"\"\n",
        "    \n",
        "    def __init__(self, feature_names):\n",
        "        self.feature_names = feature_names\n",
        "        self.importance_results = {}\n",
        "    \n",
        "    def calculate_importance(self, models_dict, X_test, y_test):\n",
        "        \"\"\"Calculate feature importance for multiple models.\"\"\"\n",
        "        \n",
        "        print(\"üîç Calculating feature importance...\")\n",
        "        \n",
        "        for name, model_info in models_dict.items():\n",
        "            model = model_info['model']\n",
        "            \n",
        "            # Different importance calculation methods based on model type\n",
        "            if hasattr(model, 'feature_importances_'):\n",
        "                # Tree-based models\n",
        "                importance = model.feature_importances_\n",
        "            else:\n",
        "                # For linear models and others, use permutation importance\n",
        "                perm_importance = permutation_importance(\n",
        "                    model, X_test, y_test, n_repeats=10, random_state=42\n",
        "                )\n",
        "                importance = perm_importance.importances_mean\n",
        "            \n",
        "            # Create importance DataFrame\n",
        "            importance_df = pd.DataFrame({\n",
        "                'feature': self.feature_names,\n",
        "                'importance': importance\n",
        "            }).sort_values('importance', ascending=False)\n",
        "            \n",
        "            self.importance_results[name] = importance_df\n",
        "            \n",
        "            print(f\"‚úÖ {name} feature importance calculated\")\n",
        "    \n",
        "    def plot_importance_comparison(self, top_n=15):\n",
        "        \"\"\"Create comparison plots of feature importance.\"\"\"\n",
        "        \n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=(\n",
        "                'Top Features - Random Forest',\n",
        "                'Top Features - XGBoost',\n",
        "                'Feature Importance Comparison',\n",
        "                'Consensus Top Features'\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        # Plot individual model importances\n",
        "        model_positions = [('Random Forest', 1, 1), ('XGBoost', 1, 2)]\n",
        "        \n",
        "        for model_name, row, col in model_positions:\n",
        "            if model_name in self.importance_results:\n",
        "                importance_df = self.importance_results[model_name].head(top_n)\n",
        "                \n",
        "                fig.add_trace(\n",
        "                    go.Bar(x=importance_df['importance'], \n",
        "                          y=importance_df['feature'],\n",
        "                          orientation='h',\n",
        "                          name=model_name),\n",
        "                    row=row, col=col\n",
        "                )\n",
        "        \n",
        "        # Feature importance comparison\n",
        "        comparison_data = []\n",
        "        for model_name in self.importance_results:\n",
        "            importance_df = self.importance_results[model_name]\n",
        "            # Normalize importance\n",
        "            importance_df['importance_norm'] = importance_df['importance'] / importance_df['importance'].max()\n",
        "            comparison_data.append(importance_df.set_index('feature')['importance_norm'])\n",
        "        \n",
        "        comparison_df = pd.concat(comparison_data, axis=1, keys=self.importance_results.keys())\n",
        "        top_features = comparison_df.mean(axis=1).sort_values(ascending=False).head(top_n).index\n",
        "        \n",
        "        for model_name in self.importance_results:\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=comparison_df.loc[top_features, model_name],\n",
        "                          y=top_features,\n",
        "                          mode='markers',\n",
        "                          name=model_name),\n",
        "                row=2, col=1\n",
        "            )\n",
        "        \n",
        "        # Consensus top features\n",
        "        consensus_importance = comparison_df.mean(axis=1).sort_values(ascending=False).head(top_n)\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Bar(x=consensus_importance.values,\n",
        "                  y=consensus_importance.index,\n",
        "                  orientation='h',\n",
        "                  name='Consensus',\n",
        "                  marker_color='purple'),\n",
        "            row=2, col=2\n",
        "        )\n",
        "        \n",
        "        fig.update_layout(\n",
        "            title_text=\"Feature Importance Analysis\",\n",
        "            height=800,\n",
        "            showlegend=True\n",
        "        )\n",
        "        \n",
        "        fig.show()\n",
        "        \n",
        "        # Print top features\n",
        "        print(\"\\nüèÜ CONSENSUS TOP 10 FEATURES:\")\n",
        "        for i, (feature, importance) in enumerate(consensus_importance.head(10).items(), 1):\n",
        "            print(f\"{i:2d}. {feature}: {importance:.4f}\")\n",
        "        \n",
        "        return consensus_importance\n",
        "\n",
        "# Analyze feature importance\n",
        "importance_analyzer = FeatureImportanceAnalyzer(X.columns)\n",
        "\n",
        "# Combine original and optimized models\n",
        "all_models = {}\n",
        "if hasattr(model_trainer, 'results'):\n",
        "    all_models.update(model_trainer.results)\n",
        "if hasattr(optimizer, 'optimized_models'):\n",
        "    for name, model in optimizer.optimized_models.items():\n",
        "        all_models[name + ' (Optimized)'] = {'model': model}\n",
        "\n",
        "importance_analyzer.calculate_importance(all_models, X_test_scaled_df, y_test)\n",
        "consensus_features = importance_analyzer.plot_importance_comparison()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Model Evaluation and Final Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def comprehensive_model_evaluation(best_model, X_test, y_test, feature_names):\n",
        "    \"\"\"Perform comprehensive evaluation of the best model.\"\"\"\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(\"üéØ COMPREHENSIVE MODEL EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Predictions\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        'R¬≤ Score': r2_score(y_test, y_pred),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
        "        'MAE': mean_absolute_error(y_test, y_pred),\n",
        "        'MAPE': np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "    }\n",
        "    \n",
        "    print(\"\\nüìä Performance Metrics:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "    \n",
        "    # Residual analysis\n",
        "    residuals = y_test - y_pred\n",
        "    \n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=2,\n",
        "        subplot_titles=(\n",
        "            'Predicted vs Actual Values',\n",
        "            'Residual Distribution',\n",
        "            'Residuals vs Predicted',\n",
        "            'Q-Q Plot of Residuals'\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    # 1. Predicted vs Actual\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=y_test, y=y_pred, mode='markers',\n",
        "                  name='Predictions', marker=dict(color='blue', opacity=0.6)),\n",
        "        row=1, col=1\n",
        "    )\n",
        "    \n",
        "    # Perfect prediction line\n",
        "    min_val = min(y_test.min(), y_pred.min())\n",
        "    max_val = max(y_test.max(), y_pred.max())\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=[min_val, max_val], y=[min_val, max_val],\n",
        "                  mode='lines', name='Perfect',\n",
        "                  line=dict(color='red', dash='dash')),\n",
        "        row=1, col=1\n",
        "    )\n",
        "    \n",
        "    # 2. Residual distribution\n",
        "    fig.add_trace(\n",
        "        go.Histogram(x=residuals, name='Residuals',\n",
        "                    marker_color='lightcoral'),\n",
        "        row=1, col=2\n",
        "    )\n",
        "    \n",
        "    # 3. Residuals vs Predicted\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=y_pred, y=residuals, mode='markers',\n",
        "                  name='Residuals', marker=dict(color='green', opacity=0.6)),\n",
        "        row=2, col=1\n",
        "    )\n",
        "    \n",
        "    # Zero residual line\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=[min_val, max_val], y=[0, 0],\n",
        "                  mode='lines', name='Zero',\n",
        "                  line=dict(color='red', dash='dash')),\n",
        "        row=2, col=1\n",
        "    )\n",
        "    \n",
        "    # 4. Q-Q plot\n",
        "    from scipy import stats\n",
        "    qq_data = stats.probplot(residuals, dist=\"norm\")\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=qq_data[0][0], y=qq_data[0][1], mode='markers',\n",
        "                  name='Residuals', marker=dict(color='purple')),\n",
        "        row=2, col=2\n",
        "    )\n",
        "    \n",
        "    # Q-Q line\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=qq_data[0][0], y=qq_data[1][0] * qq_data[0][0] + qq_data[1][1],\n",
        "                  mode='lines', name='Normal',\n",
        "                  line=dict(color='red', dash='dash')),\n",
        "        row=2, col=2\n",
        "    )\n",
        "    \n",
        "    fig.update_layout(\n",
        "        title_text=\"Comprehensive Model Diagnostics\",\n",
        "        height=700\n",
        "    )\n",
        "    \n",
        "    fig.show()\n",
        "    \n",
        "    # Statistical tests\n",
        "    print(\"\\nüìà Statistical Tests:\")\n",
        "    print(f\"  Residual Mean: {residuals.mean():.6f}\")\n",
        "    print(f\"  Residual Std: {residuals.std():.6f}\")\n",
        "    print(f\"  Shapiro-Wilk Normality Test p-value: {stats.shapiro(residuals)[1]:.4f}\")\n",
        "    \n",
        "    return metrics, residuals\n",
        "\n",
        "# Select best model for final evaluation\n",
        "best_model_name = model_trainer.best_model\n",
        "if best_model_name + ' (Optimized)' in optimizer.optimized_models:\n",
        "    final_model = optimizer.optimized_models[best_model_name + ' (Optimized)']\n",
        "    print(f\"üéØ Using optimized {best_model_name} for final evaluation\")\n",
        "else:\n",
        "    final_model = model_trainer.results[best_model_name]['model']\n",
        "    print(f\"üéØ Using baseline {best_model_name} for final evaluation\")\n",
        "\n",
        "# Comprehensive evaluation\n",
        "final_metrics, final_residuals = comprehensive_model_evaluation(\n",
        "    final_model, X_test_scaled_df, y_test, X.columns\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Model Saving and Deployment Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_model_pipeline(final_model, feature_engineer, scaler, X_columns, metrics):\n",
        "    \"\"\"Save the complete model pipeline for deployment.\"\"\"\n",
        "    \n",
        "    import os\n",
        "    import json\n",
        "    \n",
        "    # Create models directory\n",
        "    os.makedirs('models', exist_ok=True)\n",
        "    \n",
        "    # Save model\n",
        "    model_path = 'models/carbon_sequestration_model.pkl'\n",
        "    joblib.dump(final_model, model_path)\n",
        "    \n",
        "    # Save scaler\n",
        "    scaler_path = 'models/scaler.pkl'\n",
        "    joblib.dump(scaler, scaler_path)\n",
        "    \n",
        "    # Save feature engineer\n",
        "    feature_engineer_path = 'models/feature_engineer.pkl'\n",
        "    joblib.dump(feature_engineer, feature_engineer_path)\n",
        "    \n",
        "    # Save feature names\n",
        "    feature_names_path = 'models/feature_names.json'\n",
        "    with open(feature_names_path, 'w') as f:\n",
        "        json.dump(list(X_columns), f)\n",
        "    \n",
        "    # Save model metrics\n",
        "    metrics_path = 'models/model_metrics.json'\n",
        "    with open(metrics_path, 'w') as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "    \n",
        "    # Save model info\n",
        "    model_info = {\n",
        "        'model_type': type(final_model).__name__,\n",
        "        'training_date': pd.Timestamp.now().isoformat(),\n",
        "        'feature_count': len(X_columns),\n",
        "        'performance_metrics': metrics\n",
        "    }\n",
        "    \n",
        "    model_info_path = 'models/model_info.json'\n",
        "    with open(model_info_path, 'w') as f:\n",
        "        json.dump(model_info, f, indent=2)\n",
        "    \n",
        "    print(\"‚úÖ Model pipeline saved successfully!\")\n",
        "    print(f\"üìÅ Saved files in 'models/' directory:\")\n",
        "    print(f\"   - carbon_sequestration_model.pkl (Trained model)\")\n",
        "    print(f\"   - scaler.pkl (Feature scaler)\")\n",
        "    print(f\"   - feature_engineer.pkl (Feature engineering pipeline)\")\n",
        "    print(f\"   - feature_names.json (Feature names)\")\n",
        "    print(f\"   - model_metrics.json (Performance metrics)\")\n",
        "    print(f\"   - model_info.json (Model information)\")\n",
        "    \n",
        "    return model_info\n",
        "\n",
        "# Save the complete pipeline\n",
        "model_info = save_model_pipeline(\n",
        "    final_model, feature_engineer, scaler, X.columns, final_metrics\n",
        ")\n",
        "\n",
        "# Final summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ MODEL DEVELOPMENT COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nüìä FINAL MODEL PERFORMANCE:\")\n",
        "print(f\"   Model Type: {model_info['model_type']}\")\n",
        "print(f\"   R¬≤ Score: {final_metrics['R¬≤ Score']:.4f}\")\n",
        "print(f\"   RMSE: {final_metrics['RMSE']:.4f}\")\n",
        "print(f\"   MAE: {final_metrics['MAE']:.4f}\")\n",
        "print(f\"   Number of Features: {model_info['feature_count']}\")\n",
        "print(f\"\\nüí° The model is ready for carbon sequestration prediction!\")\n",
        "print(\"   Use the saved pipeline for making predictions on new data.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
