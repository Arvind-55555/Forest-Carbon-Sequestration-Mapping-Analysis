{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 05_validation\n",
        "\n",
        "## Carbon Sequestration Model Validation\n",
        "\n",
        "**Objectives:**\n",
        "- Validate model predictions with ground truth data\n",
        "- Calculate uncertainty estimates\n",
        "- Perform spatial cross-validation\n",
        "- Compare with existing carbon maps\n",
        "- Analyze spatial autocorrelation\n",
        "- Generate comprehensive performance metrics\n",
        "\n",
        "**Validation Approaches:**\n",
        "- Hold-out validation\n",
        "- Spatial cross-validation\n",
        "- Bootstrap uncertainty estimation\n",
        "- Comparison with external datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Dependencies and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine Learning and Validation\n",
        "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.utils import resample\n",
        "import scipy.stats as stats\n",
        "from scipy import spatial\n",
        "\n",
        "# Spatial Analysis\n",
        "import geopandas as gpd\n",
        "from libpysal.weights import DistanceBand, KNN\n",
        "from esda.moran import Moran\n",
        "from esda.geary import Geary\n",
        "from splot.esda import plot_moran\n",
        "\n",
        "# Visualization\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import folium\n",
        "from folium import plugins\n",
        "\n",
        "# Model and Data\n",
        "import joblib\n",
        "import json\n",
        "\n",
        "# Statistical Analysis\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tsa.stattools import acf\n",
        "\n",
        "# Setup\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"viridis\")\n",
        "%matplotlib inline\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create output directories\n",
        "import os\n",
        "os.makedirs('outputs/validation', exist_ok=True)\n",
        "os.makedirs('outputs/uncertainty', exist_ok=True)\n",
        "os.makedirs('outputs/comparison', exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ All dependencies imported successfully\")\n",
        "print(\"‚úÖ Output directories created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data and Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load trained model and preprocessing pipeline\n",
        "try:\n",
        "    model = joblib.load('models/carbon_sequestration_model.pkl')\n",
        "    scaler = joblib.load('models/scaler.pkl')\n",
        "    feature_engineer = joblib.load('models/feature_engineer.pkl')\n",
        "    \n",
        "    with open('models/feature_names.json', 'r') as f:\n",
        "        feature_names = json.load(f)\n",
        "    \n",
        "    with open('models/model_metrics.json', 'r') as f:\n",
        "        model_metrics = json.load(f)\n",
        "    \n",
        "    print(\"‚úÖ Successfully loaded trained model and pipeline\")\n",
        "    print(f\"üìä Model type: {type(model).__name__}\")\n",
        "    print(f\"üîß Number of features: {len(feature_names)}\")\n",
        "    print(f\"üìà Previous test R¬≤: {model_metrics.get('R¬≤ Score', 'N/A')}\")\n",
        "    \n",
        "except FileNotFoundError as e:\n",
        "    print(f\"‚ùå Model files not found: {e}\")\n",
        "    print(\"üí° Please run notebook 03 (model development) first\")\n",
        "    raise\n",
        "\n",
        "# Load processed data\n",
        "try:\n",
        "    biomass_df = pd.read_csv('outputs/biomass_data_with_insights.csv')\n",
        "    json_df = pd.read_csv('outputs/json_data_with_insights.csv')\n",
        "    \n",
        "    # Combine datasets\n",
        "    validation_df = pd.concat([biomass_df, json_df], ignore_index=True)\n",
        "    \n",
        "    print(\"‚úÖ Successfully loaded validation data\")\n",
        "    print(f\"üìä Total samples: {len(validation_df)}\")\n",
        "    \n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Processed data not found. Using sample data...\")\n",
        "    # Create sample validation data\n",
        "    np.random.seed(42)\n",
        "    n_samples = 200\n",
        "    validation_df = pd.DataFrame({\n",
        "        'latitude': np.random.uniform(40.0, 45.0, n_samples),\n",
        "        'longitude': np.random.uniform(-75.0, -70.0, n_samples),\n",
        "        'biomass': np.random.normal(2.5, 1.0, n_samples).clip(0.1, 5.0),\n",
        "        'carbon_stock': np.random.normal(1.2, 0.5, n_samples).clip(0.05, 2.5)\n",
        "    })\n",
        "\n",
        "# Prepare features for validation\n",
        "X_validation, y_validation = feature_engineer.prepare_features(validation_df)\n",
        "X_validation_scaled = scaler.transform(X_validation)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_validation_scaled)\n",
        "\n",
        "# Calculate residuals\n",
        "residuals = y_validation - y_pred\n",
        "\n",
        "print(f\"\\nüìä Validation Dataset:\")\n",
        "print(f\"  Samples: {len(validation_df)}\")\n",
        "print(f\"  Features: {X_validation.shape[1]}\")\n",
        "print(f\"  Carbon stock range: {y_validation.min():.3f} - {y_validation.max():.3f} kg/m¬≤\")\n",
        "print(f\"  Predictions range: {y_pred.min():.3f} - {y_pred.max():.3f} kg/m¬≤\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Ground Truth Data Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GroundTruthValidator:\n",
        "    \"\"\"Validate model predictions against ground truth data.\"\"\"\n",
        "    \n",
        "    def __init__(self, y_true, y_pred, coordinates=None):\n",
        "        self.y_true = y_true\n",
        "        self.y_pred = y_pred\n",
        "        self.coordinates = coordinates\n",
        "        self.residuals = y_true - y_pred\n",
        "        self.metrics = {}\n",
        "    \n",
        "    def calculate_comprehensive_metrics(self):\n",
        "        \"\"\"Calculate comprehensive validation metrics.\"\"\"\n",
        "        \n",
        "        print(\"üìä Calculating comprehensive validation metrics...\")\n",
        "        \n",
        "        # Basic regression metrics\n",
        "        self.metrics['r2'] = r2_score(self.y_true, self.y_pred)\n",
        "        self.metrics['rmse'] = np.sqrt(mean_squared_error(self.y_true, self.y_pred))\n",
        "        self.metrics['mae'] = mean_absolute_error(self.y_true, self.y_pred)\n",
        "        self.metrics['mse'] = mean_squared_error(self.y_true, self.y_pred)\n",
        "        \n",
        "        # Percentage errors\n",
        "        mape = np.mean(np.abs(self.residuals / self.y_true)) * 100\n",
        "        self.metrics['mape'] = mape\n",
        "        \n",
        "        # Bias and precision\n",
        "        self.metrics['bias'] = np.mean(self.residuals)\n",
        "        self.metrics['std_residuals'] = np.std(self.residuals)\n",
        "        \n",
        "        # Efficiency metrics\n",
        "        nash_sutcliffe = 1 - (np.sum(self.residuals**2) / np.sum((self.y_true - np.mean(self.y_true))**2))\n",
        "        self.metrics['nse'] = nash_sutcliffe\n",
        "        \n",
        "        # Willmott's index of agreement\n",
        "        d1 = np.sum(self.residuals**2)\n",
        "        d2 = np.sum((np.abs(self.y_pred - np.mean(self.y_true)) + np.abs(self.y_true - np.mean(self.y_true)))**2)\n",
        "        self.metrics['ioa'] = 1 - d1/d2 if d2 != 0 else 0\n",
        "        \n",
        "        # Correlation metrics\n",
        "        self.metrics['pearson_r'] = np.corrcoef(self.y_true, self.y_pred)[0, 1]\n",
        "        self.metrics['spearman_rho'] = stats.spearmanr(self.y_true, self.y_pred)[0]\n",
        "        \n",
        "        # Statistical tests\n",
        "        self.metrics['shapiro_p'] = stats.shapiro(self.residuals)[1]\n",
        "        \n",
        "        print(\"‚úÖ Comprehensive metrics calculated\")\n",
        "        return self.metrics\n",
        "    \n",
        "    def create_validation_plots(self):\n",
        "        \"\"\"Create comprehensive validation visualizations.\"\"\"\n",
        "        \n",
        "        print(\"\\nüìà Creating validation plots...\")\n",
        "        \n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=3,\n",
        "            subplot_titles=(\n",
        "                'Predicted vs Observed',\n",
        "                'Residuals vs Predicted',\n",
        "                'Residual Distribution',\n",
        "                'Q-Q Plot of Residuals',\n",
        "                'Cumulative Distribution',\n",
        "                'Error Distribution by Quantile'\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        # 1. Predicted vs Observed\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=self.y_true, y=self.y_pred, mode='markers',\n",
        "                      name='Predictions', marker=dict(color='blue', opacity=0.6)),\n",
        "            row=1, col=1\n",
        "        )\n",
        "        \n",
        "        # Perfect prediction line\n",
        "        min_val = min(self.y_true.min(), self.y_pred.min())\n",
        "        max_val = max(self.y_true.max(), self.y_pred.max())\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=[min_val, max_val], y=[min_val, max_val],\n",
        "                      mode='lines', name='Perfect',\n",
        "                      line=dict(color='red', dash='dash')),\n",
        "            row=1, col=1\n",
        "        )\n",
        "        \n",
        "        # 2. Residuals vs Predicted\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=self.y_pred, y=self.residuals, mode='markers',\n",
        "                      name='Residuals', marker=dict(color='green', opacity=0.6)),\n",
        "            row=1, col=2\n",
        "        )\n",
        "        \n",
        "        # Zero residual line\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=[min_val, max_val], y=[0, 0],\n",
        "                      mode='lines', name='Zero',\n",
        "                      line=dict(color='red', dash='dash')),\n",
        "            row=1, col=2\n",
        "        )\n",
        "        \n",
        "        # 3. Residual Distribution\n",
        "        fig.add_trace(\n",
        "            go.Histogram(x=self.residuals, name='Residuals',\n",
        "                       marker_color='lightcoral', nbinsx=30),\n",
        "            row=1, col=3\n",
        "        )\n",
        "        \n",
        "        # 4. Q-Q Plot\n",
        "        qq_data = stats.probplot(self.residuals, dist=\"norm\")\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=qq_data[0][0], y=qq_data[0][1], mode='markers',\n",
        "                      name='Residuals', marker=dict(color='purple')),\n",
        "            row=2, col=1\n",
        "        )\n",
        "        \n",
        "        # Q-Q line\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=qq_data[0][0], y=qq_data[1][0] * qq_data[0][0] + qq_data[1][1],\n",
        "                      mode='lines', name='Normal',\n",
        "                      line=dict(color='red', dash='dash')),\n",
        "            row=2, col=1\n",
        "        )\n",
        "        \n",
        "        # 5. Cumulative Distribution\n",
        "        sorted_true = np.sort(self.y_true)\n",
        "        sorted_pred = np.sort(self.y_pred)\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=sorted_true, y=np.arange(len(sorted_true)) / len(sorted_true),\n",
        "                      mode='lines', name='Observed', line=dict(color='blue')),\n",
        "            row=2, col=2\n",
        "        )\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=sorted_pred, y=np.arange(len(sorted_pred)) / len(sorted_pred),\n",
        "                      mode='lines', name='Predicted', line=dict(color='red')),\n",
        "            row=2, col=2\n",
        "        )\n",
        "        \n",
        "        # 6. Error by quantile\n",
        "        quantiles = np.percentile(self.y_true, np.arange(0, 101, 10))\n",
        "        quantile_errors = []\n",
        "        \n",
        "        for i in range(len(quantiles)-1):\n",
        "            mask = (self.y_true >= quantiles[i]) & (self.y_true < quantiles[i+1])\n",
        "            if mask.any():\n",
        "                quantile_errors.append(np.mean(np.abs(self.residuals[mask])))\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Bar(x=[f\"Q{i+1}\" for i in range(len(quantile_errors))], \n",
        "                  y=quantile_errors, name='MAE by Quantile',\n",
        "                  marker_color='orange'),\n",
        "            row=2, col=3\n",
        "        )\n",
        "        \n",
        "        fig.update_layout(\n",
        "            title_text=\"Comprehensive Model Validation\",\n",
        "            height=800,\n",
        "            showlegend=True\n",
        "        )\n",
        "        \n",
        "        fig.show()\n",
        "        fig.write_html('outputs/validation/validation_plots.html')\n",
        "        \n",
        "        print(\"‚úÖ Validation plots saved to outputs/validation/validation_plots.html\")\n",
        "    \n",
        "    def generate_validation_report(self):\n",
        "        \"\"\"Generate comprehensive validation report.\"\"\"\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"üìä COMPREHENSIVE VALIDATION REPORT\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        # Performance metrics\n",
        "        print(f\"\\nüéØ PERFORMANCE METRICS:\")\n",
        "        print(f\"  R¬≤ Score: {self.metrics['r2']:.4f}\")\n",
        "        print(f\"  RMSE: {self.metrics['rmse']:.4f} kg/m¬≤\")\n",
        "        print(f\"  MAE: {self.metrics['mae']:.4f} kg/m¬≤\")\n",
        "        print(f\"  MAPE: {self.metrics['mape']:.2f}%\")\n",
        "        print(f\"  Nash-Sutcliffe Efficiency: {self.metrics['nse']:.4f}\")\n",
        "        print(f\"  Index of Agreement: {self.metrics['ioa']:.4f}\")\n",
        "        \n",
        "        # Bias and precision\n",
        "        print(f\"\\n‚öñÔ∏è  BIAS AND PRECISION:\")\n",
        "        print(f\"  Mean Bias: {self.metrics['bias']:.4f} kg/m¬≤\")\n",
        "        print(f\"  Residual Std: {self.metrics['std_residuals']:.4f} kg/m¬≤\")\n",
        "        print(f\"  Pearson Correlation: {self.metrics['pearson_r']:.4f}\")\n",
        "        print(f\"  Spearman Correlation: {self.metrics['spearman_rho']:.4f}\")\n",
        "        \n",
        "        # Statistical tests\n",
        "        print(f\"\\nüìà STATISTICAL TESTS:\")\n",
        "        normality = \"Normal\" if self.metrics['shapiro_p'] > 0.05 else \"Non-normal\"\n",
        "        print(f\"  Residual Normality (Shapiro-Wilk): {normality} (p={self.metrics['shapiro_p']:.4f})\")\n",
        "        \n",
        "        # Performance interpretation\n",
        "        print(f\"\\nüí° PERFORMANCE INTERPRETATION:\")\n",
        "        if self.metrics['r2'] > 0.8:\n",
        "            print(\"  ‚úÖ Excellent predictive performance\")\n",
        "        elif self.metrics['r2'] > 0.6:\n",
        "            print(\"  ‚úÖ Good predictive performance\")\n",
        "        elif self.metrics['r2'] > 0.4:\n",
        "            print(\"  ‚ö†Ô∏è  Moderate predictive performance\")\n",
        "        else:\n",
        "            print(\"  ‚ùå Poor predictive performance\")\n",
        "        \n",
        "        if abs(self.metrics['bias']) < 0.1:\n",
        "            print(\"  ‚úÖ Low bias in predictions\")\n",
        "        else:\n",
        "            print(\"  ‚ö†Ô∏è  Noticeable bias in predictions\")\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Perform ground truth validation\n",
        "validator = GroundTruthValidator(y_validation, y_pred)\n",
        "metrics = validator.calculate_comprehensive_metrics()\n",
        "validator.create_validation_plots()\n",
        "validator.generate_validation_report()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Uncertainty Quantification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class UncertaintyQuantifier:\n",
        "    \"\"\"Quantify prediction uncertainty using multiple methods.\"\"\"\n",
        "    \n",
        "    def __init__(self, model, X, y, feature_names, n_bootstrap=100):\n",
        "        self.model = model\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.feature_names = feature_names\n",
        "        self.n_bootstrap = n_bootstrap\n",
        "        self.bootstrap_predictions = None\n",
        "        self.uncertainty_metrics = {}\n",
        "    \n",
        "    def bootstrap_uncertainty(self):\n",
        "        \"\"\"Estimate uncertainty using bootstrap resampling.\"\"\"\n",
        "        \n",
        "        print(\"üîÑ Calculating bootstrap uncertainty...\")\n",
        "        \n",
        "        bootstrap_predictions = []\n",
        "        bootstrap_models = []\n",
        "        \n",
        "        for i in range(self.n_bootstrap):\n",
        "            # Bootstrap sample\n",
        "            X_resampled, y_resampled = resample(self.X, self.y, random_state=i)\n",
        "            \n",
        "            # Clone and train model\n",
        "            model_clone = joblib.load('models/carbon_sequestration_model.pkl')\n",
        "            if hasattr(model_clone, 'random_state'):\n",
        "                model_clone.random_state = i\n",
        "            \n",
        "            model_clone.fit(X_resampled, y_resampled)\n",
        "            \n",
        "            # Predict on original data\n",
        "            y_pred_bootstrap = model_clone.predict(self.X)\n",
        "            bootstrap_predictions.append(y_pred_bootstrap)\n",
        "            bootstrap_models.append(model_clone)\n",
        "            \n",
        "            if (i + 1) % 20 == 0:\n",
        "                print(f\"  Completed {i + 1}/{self.n_bootstrap} bootstrap iterations\")\n",
        "        \n",
        "        self.bootstrap_predictions = np.array(bootstrap_predictions)\n",
        "        self.bootstrap_models = bootstrap_models\n",
        "        \n",
        "        # Calculate uncertainty metrics\n",
        "        self.uncertainty_metrics['prediction_mean'] = np.mean(self.bootstrap_predictions, axis=0)\n",
        "        self.uncertainty_metrics['prediction_std'] = np.std(self.bootstrap_predictions, axis=0)\n",
        "        self.uncertainty_metrics['prediction_ci_lower'] = np.percentile(self.bootstrap_predictions, 2.5, axis=0)\n",
        "        self.uncertainty_metrics['prediction_ci_upper'] = np.percentile(self.bootstrap_predictions, 97.5, axis=0)\n",
        "        \n",
        "        print(\"‚úÖ Bootstrap uncertainty analysis completed\")\n",
        "        return self.uncertainty_metrics\n",
        "    \n",
        "    def calculate_prediction_intervals(self, confidence=0.95):\n",
        "        \"\"\"Calculate prediction intervals for model outputs.\"\"\"\n",
        "        \n",
        "        if self.bootstrap_predictions is None:\n",
        "            self.bootstrap_uncertainty()\n",
        "        \n",
        "        alpha = 1 - confidence\n",
        "        lower_percentile = (alpha / 2) * 100\n",
        "        upper_percentile = (1 - alpha / 2) * 100\n",
        "        \n",
        "        prediction_intervals = {\n",
        "            'lower': np.percentile(self.bootstrap_predictions, lower_percentile, axis=0),\n",
        "            'upper': np.percentile(self.bootstrap_predictions, upper_percentile, axis=0),\n",
        "            'mean': self.uncertainty_metrics['prediction_mean']\n",
        "        }\n",
        "        \n",
        "        # Calculate coverage\n",
        "        coverage = np.mean((self.y >= prediction_intervals['lower']) & \n",
        "                          (self.y <= prediction_intervals['upper']))\n",
        "        \n",
        "        self.uncertainty_metrics['coverage'] = coverage\n",
        "        self.uncertainty_metrics['interval_width'] = np.mean(prediction_intervals['upper'] - prediction_intervals['lower'])\n",
        "        \n",
        "        print(f\"‚úÖ Prediction intervals calculated ({confidence*100}% confidence)\")\n",
        "        print(f\"   Coverage: {coverage:.3f} (target: {confidence})\")\n",
        "        print(f\"   Average interval width: {self.uncertainty_metrics['interval_width']:.4f} kg/m¬≤\")\n",
        "        \n",
        "        return prediction_intervals\n",
        "    \n",
        "    def create_uncertainty_visualizations(self):\n",
        "        \"\"\"Create visualizations of prediction uncertainty.\"\"\"\n",
        "        \n",
        "        if self.bootstrap_predictions is None:\n",
        "            self.bootstrap_uncertainty()\n",
        "        \n",
        "        print(\"\\nüìä Creating uncertainty visualizations...\")\n",
        "        \n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=(\n",
        "                'Prediction Uncertainty',\n",
        "                'Confidence Intervals Coverage',\n",
        "                'Uncertainty Distribution',\n",
        "                'Spatial Uncertainty Pattern'\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        # 1. Prediction uncertainty\n",
        "        sorted_idx = np.argsort(self.y)\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=np.arange(len(self.y)), y=self.uncertainty_metrics['prediction_mean'][sorted_idx],\n",
        "                      mode='lines', name='Mean Prediction', line=dict(color='blue')),\n",
        "            row=1, col=1\n",
        "        )\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=np.arange(len(self.y)), \n",
        "                      y=self.uncertainty_metrics['prediction_ci_upper'][sorted_idx],\n",
        "                      mode='lines', name='95% CI Upper', line=dict(color='lightblue', dash='dash')),\n",
        "            row=1, col=1\n",
        "        )\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=np.arange(len(self.y)), \n",
        "                      y=self.uncertainty_metrics['prediction_ci_lower'][sorted_idx],\n",
        "                      mode='lines', name='95% CI Lower', line=dict(color='lightblue', dash='dash'),\n",
        "                      fill='tonexty'),\n",
        "            row=1, col=1\n",
        "        )\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=np.arange(len(self.y)), y=self.y[sorted_idx],\n",
        "                      mode='markers', name='Observed', marker=dict(color='red')),\n",
        "            row=1, col=1\n",
        "        )\n",
        "        \n",
        "        # 2. Confidence intervals coverage\n",
        "        coverage_by_quantile = []\n",
        "        quantiles = np.percentile(self.y, np.arange(0, 101, 10))\n",
        "        \n",
        "        for i in range(len(quantiles)-1):\n",
        "            mask = (self.y >= quantiles[i]) & (self.y < quantiles[i+1])\n",
        "            if mask.any():\n",
        "                coverage = np.mean((self.y[mask] >= self.uncertainty_metrics['prediction_ci_lower'][mask]) & \n",
        "                                 (self.y[mask] <= self.uncertainty_metrics['prediction_ci_upper'][mask]))\n",
        "                coverage_by_quantile.append(coverage)\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Bar(x=[f\"Q{i+1}\" for i in range(len(coverage_by_quantile))], \n",
        "                  y=coverage_by_quantile, name='Coverage by Quantile',\n",
        "                  marker_color='lightgreen'),\n",
        "            row=1, col=2\n",
        "        )\n",
        "        \n",
        "        # Add target coverage line\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=[-0.5, len(coverage_by_quantile)-0.5], y=[0.95, 0.95],\n",
        "                      mode='lines', name='Target (95%)',\n",
        "                      line=dict(color='red', dash='dash')),\n",
        "            row=1, col=2\n",
        "        )\n",
        "        \n",
        "        # 3. Uncertainty distribution\n",
        "        fig.add_trace(\n",
        "            go.Histogram(x=self.uncertainty_metrics['prediction_std'], \n",
        "                       name='Uncertainty Distribution', nbinsx=30,\n",
        "                       marker_color='orange'),\n",
        "            row=2, col=1\n",
        "        )\n",
        "        \n",
        "        # 4. Spatial uncertainty pattern (if coordinates available)\n",
        "        if hasattr(self, 'coordinates'):\n",
        "            fig.add_trace(\n",
        "                go.Scatter(x=self.coordinates[:, 0], y=self.coordinates[:, 1],\n",
        "                          mode='markers', \n",
        "                          marker=dict(size=8, color=self.uncertainty_metrics['prediction_std'],\n",
        "                                    colorscale='Viridis', showscale=True),\n",
        "                          name='Spatial Uncertainty'),\n",
        "                row=2, col=2\n",
        "            )\n",
        "        \n",
        "        fig.update_layout(\n",
        "            title_text=\"Prediction Uncertainty Analysis\",\n",
        "            height=700\n",
        "        )\n",
        "        \n",
        "        fig.show()\n",
        "        fig.write_html('outputs/uncertainty/uncertainty_analysis.html')\n",
        "        \n",
        "        print(\"‚úÖ Uncertainty visualizations saved to outputs/uncertainty/uncertainty_analysis.html\")\n",
        "    \n",
        "    def generate_uncertainty_report(self):\n",
        "        \"\"\"Generate comprehensive uncertainty report.\"\"\"\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"üìä UNCERTAINTY QUANTIFICATION REPORT\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        print(f\"\\nüéØ UNCERTAINTY METRICS:\")\n",
        "        print(f\"  Mean prediction uncertainty: {np.mean(self.uncertainty_metrics['prediction_std']):.4f} kg/m¬≤\")\n",
        "        print(f\"  Std of prediction uncertainty: {np.std(self.uncertainty_metrics['prediction_std']):.4f} kg/m¬≤\")\n",
        "        print(f\"  95% confidence interval coverage: {self.uncertainty_metrics.get('coverage', 'N/A'):.3f}\")\n",
        "        print(f\"  Average prediction interval width: {self.uncertainty_metrics.get('interval_width', 'N/A'):.4f} kg/m¬≤\")\n",
        "        \n",
        "        # Uncertainty interpretation\n",
        "        mean_uncertainty = np.mean(self.uncertainty_metrics['prediction_std'])\n",
        "        data_std = np.std(self.y)\n",
        "        uncertainty_ratio = mean_uncertainty / data_std\n",
        "        \n",
        "        print(f\"\\nüí° UNCERTAINTY INTERPRETATION:\")\n",
        "        print(f\"  Uncertainty-to-variability ratio: {uncertainty_ratio:.3f}\")\n",
        "        \n",
        "        if uncertainty_ratio < 0.1:\n",
        "            print(\"  ‚úÖ Low uncertainty relative to data variability\")\n",
        "        elif uncertainty_ratio < 0.3:\n",
        "            print(\"  ‚úÖ Moderate uncertainty relative to data variability\")\n",
        "        else:\n",
        "            print(\"  ‚ö†Ô∏è  High uncertainty relative to data variability\")\n",
        "        \n",
        "        if self.uncertainty_metrics.get('coverage', 0) > 0.9:\n",
        "            print(\"  ‚úÖ Good confidence interval coverage\")\n",
        "        else:\n",
        "            print(\"  ‚ö†Ô∏è  Confidence interval coverage could be improved\")\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Perform uncertainty quantification\n",
        "uncertainty_analyzer = UncertaintyQuantifier(model, X_validation_scaled, y_validation, feature_names)\n",
        "uncertainty_metrics = uncertainty_analyzer.bootstrap_uncertainty()\n",
        "prediction_intervals = uncertainty_analyzer.calculate_prediction_intervals()\n",
        "uncertainty_analyzer.create_uncertainty_visualizations()\n",
        "uncertainty_analyzer.generate_uncertainty_report()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Spatial Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SpatialValidator:\n",
        "    \"\"\"Perform spatial cross-validation and autocorrelation analysis.\"\"\"\n",
        "    \n",
        "    def __init__(self, coordinates, y_true, y_pred, model, X):\n",
        "        self.coordinates = coordinates\n",
        "        self.y_true = y_true\n",
        "        self.y_pred = y_pred\n",
        "        self.model = model\n",
        "        self.X = X\n",
        "        self.spatial_metrics = {}\n",
        "    \n",
        "    def spatial_cross_validation(self, n_folds=5, distance_threshold=0.1):\n",
        "        \"\"\"Perform spatial cross-validation using distance-based folds.\"\"\"\n",
        "        \n",
        "        print(\"üó∫Ô∏è Performing spatial cross-validation...\")\n",
        "        \n",
        "        from sklearn.model_selection import cross_val_score\n",
        "        \n",
        "        # Create spatial folds using K-means on coordinates\n",
        "        from sklearn.cluster import KMeans\n",
        "        \n",
        "        kmeans = KMeans(n_clusters=n_folds, random_state=42)\n",
        "        spatial_folds = kmeans.fit_predict(self.coordinates)\n",
        "        \n",
        "        # Perform cross-validation\n",
        "        spatial_cv_scores = []\n",
        "        spatial_rmse_scores = []\n",
        "        \n",
        "        for fold in range(n_folds):\n",
        "            # Create train/test masks\n",
        "            test_mask = spatial_folds == fold\n",
        "            train_mask = ~test_mask\n",
        "            \n",
        "            if train_mask.sum() == 0 or test_mask.sum() == 0:\n",
        "                continue\n",
        "            \n",
        "            # Train model\n",
        "            model_clone = joblib.load('models/carbon_sequestration_model.pkl')\n",
        "            model_clone.fit(self.X[train_mask], self.y_true[train_mask])\n",
        "            \n",
        "            # Predict and score\n",
        "            y_pred_fold = model_clone.predict(self.X[test_mask])\n",
        "            \n",
        "            r2_fold = r2_score(self.y_true[test_mask], y_pred_fold)\n",
        "            rmse_fold = np.sqrt(mean_squared_error(self.y_true[test_mask], y_pred_fold))\n",
        "            \n",
        "            spatial_cv_scores.append(r2_fold)\n",
        "            spatial_rmse_scores.append(rmse_fold)\n",
        "            \n",
        "            print(f\"  Fold {fold + 1}: R¬≤ = {r2_fold:.4f}, RMSE = {rmse_fold:.4f}\")\n",
        "        \n",
        "        self.spatial_metrics['spatial_cv_r2_mean'] = np.mean(spatial_cv_scores)\n",
        "        self.spatial_metrics['spatial_cv_r2_std'] = np.std(spatial_cv_scores)\n",
        "        self.spatial_metrics['spatial_cv_rmse_mean'] = np.mean(spatial_rmse_scores)\n",
        "        self.spatial_metrics['spatial_cv_rmse_std'] = np.std(spatial_rmse_scores)\n",
        "        \n",
        "        print(\"‚úÖ Spatial cross-validation completed\")\n",
        "        return spatial_cv_scores, spatial_rmse_scores\n",
        "    \n",
        "    def analyze_spatial_autocorrelation(self):\n",
        "        \"\"\"Analyze spatial autocorrelation in residuals.\"\"\"\n",
        "        \n",
        "        print(\"\\nüîç Analyzing spatial autocorrelation...\")\n",
        "        \n",
        "        residuals = self.y_true - self.y_pred\n",
        "        \n",
        "        # Calculate Moran's I\n",
        "        try:\n",
        "            # Create spatial weights matrix\n",
        "            w = DistanceBand(self.coordinates, threshold=0.1, binary=False)\n",
        "            \n",
        "            # Moran's I for residuals\n",
        "            moran_residuals = Moran(residuals, w)\n",
        "            \n",
        "            # Moran's I for original data\n",
        "            moran_original = Moran(self.y_true, w)\n",
        "            \n",
        "            self.spatial_metrics['moran_residuals'] = moran_residuals.I\n",
        "            self.spatial_metrics['moran_residuals_p'] = moran_residuals.p_sim\n",
        "            self.spatial_metrics['moran_original'] = moran_original.I\n",
        "            self.spatial_metrics['moran_original_p'] = moran_original.p_sim\n",
        "            \n",
        "            print(f\"  Moran's I (Residuals): {moran_residuals.I:.4f} (p={moran_residuals.p_sim:.4f})\")\n",
        "            print(f\"  Moran's I (Original): {moran_original.I:.4f} (p={moran_original.p_sim:.4f})\")\n",
        "            \n",
        "            # Create spatial autocorrelation plot\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "            \n",
        "            # Moran plot for residuals\n",
        "            plot_moran(moran_residuals, ax=ax1)\n",
        "            ax1.set_title(f\"Moran's I - Residuals\\nI = {moran_residuals.I:.4f}, p = {moran_residuals.p_sim:.4f}\")\n",
        "            \n",
        "            # Moran plot for original data\n",
        "            plot_moran(moran_original, ax=ax2)\n",
        "            ax2.set_title(f\"Moran's I - Original Data\\nI = {moran_original.I:.4f}, p = {moran_original.p_sim:.4f}\")\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.savefig('outputs/validation/spatial_autocorrelation.png', dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Spatial autocorrelation analysis failed: {e}\")\n",
        "            print(\"üí° This might be due to insufficient spatial variation or computational limitations\")\n",
        "    \n",
        "    def create_spatial_validation_plots(self):\n",
        "        \"\"\"Create spatial validation visualizations.\"\"\"\n",
        "        \n",
        "        print(\"\\nüìä Creating spatial validation plots...\")\n",
        "        \n",
        "        residuals = self.y_true - self.y_pred\n",
        "        \n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=(\n",
        "                'Spatial Distribution of Residuals',\n",
        "                'Residuals vs Spatial Coordinates',\n",
        "                'Spatial Error Clustering',\n",
        "                'Distance vs Prediction Error'\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        # 1. Spatial distribution of residuals\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=self.coordinates[:, 0], y=self.coordinates[:, 1],\n",
        "                      mode='markers', \n",
        "                      marker=dict(size=8, color=residuals, \n",
        "                                colorscale='RdBu', \n",
        "                                colorbar=dict(title='Residuals'),\n",
        "                                showscale=True),\n",
        "                      name='Residuals'),\n",
        "            row=1, col=1\n",
        "        )\n",
        "        \n",
        "        # 2. Residuals vs latitude\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=self.coordinates[:, 1], y=residuals,\n",
        "                      mode='markers', name='Residuals vs Lat',\n",
        "                      marker=dict(color='blue', opacity=0.6)),\n",
        "            row=1, col=2\n",
        "        )\n",
        "        \n",
        "        # 3. Spatial error clustering\n",
        "        from sklearn.cluster import DBSCAN\n",
        "        \n",
        "        # Identify error clusters\n",
        "        error_magnitude = np.abs(residuals)\n",
        "        high_error_mask = error_magnitude > np.percentile(error_magnitude, 75)\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=self.coordinates[~high_error_mask, 0], \n",
        "                      y=self.coordinates[~high_error_mask, 1],\n",
        "                      mode='markers', name='Low Error',\n",
        "                      marker=dict(color='green', size=6, opacity=0.6)),\n",
        "            row=2, col=1\n",
        "        )\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=self.coordinates[high_error_mask, 0], \n",
        "                      y=self.coordinates[high_error_mask, 1],\n",
        "                      mode='markers', name='High Error',\n",
        "                      marker=dict(color='red', size=8, opacity=0.8)),\n",
        "            row=2, col=1\n",
        "        )\n",
        "        \n",
        "        # 4. Distance vs prediction error\n",
        "        center = np.mean(self.coordinates, axis=0)\n",
        "        distances = np.sqrt(np.sum((self.coordinates - center) ** 2, axis=1))\n",
        "        \n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=distances, y=error_magnitude,\n",
        "                      mode='markers', name='Error vs Distance',\n",
        "                      marker=dict(color='purple', opacity=0.6)),\n",
        "            row=2, col=2\n",
        "        )\n",
        "        \n",
        "        fig.update_layout(\n",
        "            title_text=\"Spatial Validation Analysis\",\n",
        "            height=700\n",
        "        )\n",
        "        \n",
        "        fig.show()\n",
        "        fig.write_html('outputs/validation/spatial_validation.html')\n",
        "        \n",
        "        print(\"‚úÖ Spatial validation plots saved to outputs/validation/spatial_validation.html\")\n",
        "    \n",
        "    def generate_spatial_validation_report(self):\n",
        "        \"\"\"Generate spatial validation report.\"\"\"\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"üó∫Ô∏è SPATIAL VALIDATION REPORT\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        print(f\"\\nüéØ SPATIAL CROSS-VALIDATION:\")\n",
        "        print(f\"  Mean R¬≤: {self.spatial_metrics.get('spatial_cv_r2_mean', 'N/A'):.4f}\")\n",
        "        print(f\"  Std R¬≤: {self.spatial_metrics.get('spatial_cv_r2_std', 'N/A'):.4f}\")\n",
        "        print(f\"  Mean RMSE: {self.spatial_metrics.get('spatial_cv_rmse_mean', 'N/A'):.4f} kg/m¬≤\")\n",
        "        \n",
        "        print(f\"\\nüîç SPATIAL AUTOCORRELATION:\")\n",
        "        moran_residuals = self.spatial_metrics.get('moran_residuals', None)\n",
        "        moran_original = self.spatial_metrics.get('moran_original', None)\n",
        "        \n",
        "        if moran_residuals is not None:\n",
        "            print(f\"  Moran's I (Residuals): {moran_residuals:.4f}\")\n",
        "            print(f\"  Moran's I (Original): {moran_original:.4f}\")\n",
        "            \n",
        "            # Interpretation\n",
        "            if abs(moran_residuals) < 0.1:\n",
        "                print(\"  ‚úÖ Low spatial autocorrelation in residuals\")\n",
        "            elif abs(moran_residuals) < 0.3:\n",
        "                print(\"  ‚ö†Ô∏è  Moderate spatial autocorrelation in residuals\")\n",
        "            else:\n",
        "                print(\"  ‚ùå High spatial autocorrelation in residuals\")\n",
        "            \n",
        "            if moran_residuals < moran_original:\n",
        "                print(\"  ‚úÖ Model captures some spatial structure\")\n",
        "            else:\n",
        "                print(\"  ‚ö†Ô∏è  Model may not fully capture spatial structure\")\n",
        "        \n",
        "        print(f\"\\nüí° SPATIAL PERFORMANCE INSIGHTS:\")\n",
        "        spatial_cv_r2 = self.spatial_metrics.get('spatial_cv_r2_mean', 0)\n",
        "        regular_r2 = r2_score(self.y_true, self.y_pred)\n",
        "        \n",
        "        if spatial_cv_r2 > regular_r2 * 0.8:\n",
        "            print(\"  ‚úÖ Good spatial generalization\")\n",
        "        else:\n",
        "            print(\"  ‚ö†Ô∏è  Potential spatial overfitting\")\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Prepare coordinates for spatial analysis\n",
        "coordinates = validation_df[['longitude', 'latitude']].values\n",
        "\n",
        "# Perform spatial validation\n",
        "spatial_validator = SpatialValidator(coordinates, y_validation, y_pred, model, X_validation_scaled)\n",
        "cv_scores, rmse_scores = spatial_validator.spatial_cross_validation()\n",
        "spatial_validator.analyze_spatial_autocorrelation()\n",
        "spatial_validator.create_spatial_validation_plots()\n",
        "spatial_validator.generate_spatial_validation_report()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Comparison with Existing Carbon Maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CarbonMapComparator:\n",
        "    \"\"\"Compare model predictions with existing carbon maps.\"\"\"\n",
        "    \n",
        "    def __init__(self, validation_df, y_pred):\n",
        "        self.validation_df = validation_df\n",
        "        self.y_pred = y_pred\n",
        "        self.comparison_results = {}\n",
        "    \n",
        "    def simulate_external_datasets(self):\n",
        "        \"\"\"Simulate comparison with external carbon datasets.\"\"\"\n",
        "        \n",
        "        print(\"üåç Simulating comparison with external carbon maps...\")\n",
        "        \n",
        "        # Simulate different external datasets\n",
        "        np.random.seed(42)\n",
        "        \n",
        "        # Dataset 1: Global carbon map (coarser resolution)\n",
        "        global_carbon = self.validation_df['carbon_stock'].values + np.random.normal(0, 0.3, len(self.validation_df))\n",
        "        \n",
        "        # Dataset 2: Regional assessment (similar resolution)\n",
        "        regional_carbon = self.validation_df['carbon_stock'].values + np.random.normal(0, 0.2, len(self.validation_df))\n",
        "        \n",
        "        # Dataset 3: Satellite-based estimate\n",
        "        satellite_carbon = self.validation_df['carbon_stock'].values + np.random.normal(0, 0.25, len(self.validation_df))\n",
        "        \n",
        "        external_datasets = {\n",
        "            'Global_Carbon_Map': global_carbon,\n",
        "            'Regional_Assessment': regional_carbon,\n",
        "            'Satellite_Estimate': satellite_carbon\n",
        "        }\n",
        "        \n",
        "        self.external_datasets = external_datasets\n",
        "        print(\"‚úÖ External datasets simulated for comparison\")\n",
        "        \n",
        "        return external_datasets\n",
        "    \n",
        "    def compare_with_external_data(self):\n",
        "        \"\"\"Compare model predictions with external datasets.\"\"\"\n",
        "        \n",
        "        if not hasattr(self, 'external_datasets'):\n",
        "            self.simulate_external_datasets()\n",
        "        \n",
        "        print(\"\\nüìä Comparing with external datasets...\")\n",
        "        \n",
        "        comparison_metrics = {}\n",
        "        \n",
        "        for dataset_name, external_data in self.external_datasets.items():\n",
        "            # Calculate comparison metrics\n",
        "            r2 = r2_score(self.validation_df['carbon_stock'], external_data)\n",
        "            rmse = np.sqrt(mean_squared_error(self.validation_df['carbon_stock'], external_data))\n",
        "            mae = mean_absolute_error(self.validation_df['carbon_stock'], external_data)\n",
        "            bias = np.mean(external_data - self.validation_df['carbon_stock'])\n",
        "            \n",
        "            comparison_metrics[dataset_name] = {\n",
        "                'r2': r2,\n",
        "                'rmse': rmse,\n",
        "                'mae': mae,\n",
        "                'bias': bias,\n",
        "                'correlation': np.corrcoef(self.validation_df['carbon_stock'], external_data)[0, 1]\n",
        "            }\n",
        "            \n",
        "            print(f\"  {dataset_name}:\")\n",
        "            print(f\"    R¬≤: {r2:.4f}, RMSE: {rmse:.4f}, Bias: {bias:.4f}\")\n",
        "        \n",
        "        # Compare with our model\n",
        "        model_metrics = {\n",
        "            'r2': r2_score(self.validation_df['carbon_stock'], self.y_pred),\n",
        "            'rmse': np.sqrt(mean_squared_error(self.validation_df['carbon_stock'], self.y_pred)),\n",
        "            'mae': mean_absolute_error(self.validation_df['carbon_stock'], self.y_pred),\n",
        "            'bias': np.mean(self.y_pred - self.validation_df['carbon_stock']),\n",
        "            'correlation': np.corrcoef(self.validation_df['carbon_stock'], self.y_pred)[0, 1]\n",
        "        }\n",
        "        \n",
        "        comparison_metrics['Our_Model'] = model_metrics\n",
        "        self.comparison_results = comparison_metrics\n",
        "        \n",
        "        return comparison_metrics\n",
        "    \n",
        "    def create_comparison_visualizations(self):\n",
        "        \"\"\"Create visualizations comparing different carbon estimates.\"\"\"\n",
        "        \n",
        "        if not self.comparison_results:\n",
        "            self.compare_with_external_data()\n",
        "        \n",
        "        print(\"\\nüìà Creating comparison visualizations...\")\n",
        "        \n",
        "        # Create comparison DataFrame\n",
        "        comparison_data = []\n",
        "        \n",
        "        for dataset_name, metrics in self.comparison_results.items():\n",
        "            comparison_data.append({\n",
        "                'Dataset': dataset_name,\n",
        "                'R¬≤': metrics['r2'],\n",
        "                'RMSE': metrics['rmse'],\n",
        "                'MAE': metrics['mae'],\n",
        "                'Bias': metrics['bias'],\n",
        "                'Correlation': metrics['correlation']\n",
        "            })\n",
        "        \n",
        "        comparison_df = pd.DataFrame(comparison_data)\n",
        "        \n",
        "        # Create radar chart for comparison\n",
        "        fig = go.Figure()\n",
        "        \n",
        "        metrics_to_plot = ['R¬≤', 'Correlation']  # Positive metrics (higher is better)\n",
        "        \n",
        "        for dataset_name in comparison_df['Dataset']:\n",
        "            dataset_metrics = comparison_df[comparison_df['Dataset'] == dataset_name].iloc[0]\n",
        "            \n",
        "            values = [dataset_metrics[metric] for metric in metrics_to_plot]\n",
        "            # Add first value at end to close the radar chart\n",
        "            values.append(values[0])\n",
        "            \n",
        "            fig.add_trace(go.Scatterpolar(\n",
        "                r=values,\n",
        "                theta=metrics_to_plot + [metrics_to_plot[0]],\n",
        "                fill='toself',\n",
        "                name=dataset_name\n",
        "            ))\n",
        "        \n",
        "        fig.update_layout(\n",
        "            polar=dict(\n",
        "                radialaxis=dict(\n",
        "                    visible=True,\n",
        "                    range=[0, 1]\n",
        "                )),\n",
        "            showlegend=True,\n",
        "            title=\"Carbon Map Comparison - Performance Metrics\"\n",
        "        )\n",
        "        \n",
        "        fig.show()\n",
        "        \n",
        "        # Create bar chart comparison\n",
        "        fig_bar = px.bar(comparison_df, x='Dataset', y=['R¬≤', 'Correlation'], \n",
        "                        title='Carbon Map Performance Comparison',\n",
        "                        barmode='group')\n",
        "        \n",
        "        fig_bar.show()\n",
        "        \n",
        "        # Save comparison results\n",
        "        comparison_df.to_csv('outputs/comparison/carbon_map_comparison.csv', index=False)\n",
        "        \n",
        "        print(\"‚úÖ Comparison results saved to outputs/comparison/carbon_map_comparison.csv\")\n",
        "    \n",
        "    def generate_comparison_report(self):\n",
        "        \"\"\"Generate comparison report with external datasets.\"\"\"\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"üåç EXTERNAL DATASET COMPARISON REPORT\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        if not self.comparison_results:\n",
        "            self.compare_with_external_data()\n",
        "        \n",
        "        # Find best performing dataset\n",
        "        best_r2 = -np.inf\n",
        "        best_dataset = \"\"\n",
        "        \n",
        "        for dataset_name, metrics in self.comparison_results.items():\n",
        "            if metrics['r2'] > best_r2:\n",
        "                best_r2 = metrics['r2']\n",
        "                best_dataset = dataset_name\n",
        "        \n",
        "        print(f\"\\nüèÜ BEST PERFORMING DATASET: {best_dataset} (R¬≤ = {best_r2:.4f})\")\n",
        "        \n",
        "        # Our model's performance\n",
        "        our_model_metrics = self.comparison_results.get('Our_Model', {})\n",
        "        \n",
        "        print(f\"\\nüéØ OUR MODEL PERFORMANCE:\")\n",
        "        print(f\"  R¬≤: {our_model_metrics.get('r2', 'N/A'):.4f}\")\n",
        "        print(f\"  Rank: {sorted([m['r2'] for m in self.comparison_results.values()], reverse=True).index(our_model_metrics.get('r2', 0)) + 1} of {len(self.comparison_results)}\")\n",
        "        \n",
        "        # Improvement over external datasets\n",
        "        print(f\"\\nüìà PERFORMANCE COMPARISON:\")\n",
        "        for dataset_name, metrics in self.comparison_results.items():\n",
        "            if dataset_name != 'Our_Model':\n",
        "                improvement = our_model_metrics.get('r2', 0) - metrics.get('r2', 0)\n",
        "                print(f\"  vs {dataset_name}: {improvement:+.4f} R¬≤ improvement\")\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Perform comparison with external datasets\n",
        "comparator = CarbonMapComparator(validation_df, y_pred)\n",
        "comparison_metrics = comparator.compare_with_external_data()\n",
        "comparator.create_comparison_visualizations()\n",
        "comparator.generate_comparison_report()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Final Validation Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_comprehensive_validation_summary():\n",
        "    \"\"\"Generate comprehensive validation summary report.\"\"\"\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(\"üìä COMPREHENSIVE VALIDATION SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Collect all validation results\n",
        "    summary_data = {}\n",
        "    \n",
        "    # Ground truth validation\n",
        "    if 'validator' in locals():\n",
        "        summary_data['Ground Truth R¬≤'] = validator.metrics.get('r2', 'N/A')\n",
        "        summary_data['Ground Truth RMSE'] = validator.metrics.get('rmse', 'N/A')\n",
        "    \n",
        "    # Uncertainty metrics\n",
        "    if 'uncertainty_analyzer' in locals():\n",
        "        summary_data['Mean Uncertainty'] = np.mean(uncertainty_analyzer.uncertainty_metrics.get('prediction_std', 0))\n",
        "        summary_data['CI Coverage'] = uncertainty_analyzer.uncertainty_metrics.get('coverage', 'N/A')\n",
        "    \n",
        "    # Spatial validation\n",
        "    if 'spatial_validator' in locals():\n",
        "        summary_data['Spatial CV R¬≤'] = spatial_validator.spatial_metrics.get('spatial_cv_r2_mean', 'N/A')\n",
        "        summary_data['Moran\\'s I (Residuals)'] = spatial_validator.spatial_metrics.get('moran_residuals', 'N/A')\n",
        "    \n",
        "    # External comparison\n",
        "    if 'comparator' in locals():\n",
        "        our_model_r2 = comparator.comparison_results.get('Our_Model', {}).get('r2', 'N/A')\n",
        "        summary_data['External Comparison R¬≤'] = our_model_r2\n",
        "    \n",
        "    # Create summary table\n",
        "    summary_df = pd.DataFrame(list(summary_data.items()), columns=['Metric', 'Value'])\n",
        "    \n",
        "    print(\"\\nüìà VALIDATION METRICS SUMMARY:\")\n",
        "    for metric, value in summary_data.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f\"  {metric:30} {value:.4f}\")\n",
        "        else:\n",
        "            print(f\"  {metric:30} {value}\")\n",
        "    \n",
        "    # Overall assessment\n",
        "    print(f\"\\nüí° OVERALL ASSESSMENT:\")\n",
        "    \n",
        "    ground_truth_r2 = summary_data.get('Ground Truth R¬≤', 0)\n",
        "    spatial_cv_r2 = summary_data.get('Spatial CV R¬≤', 0)\n",
        "    moran_residuals = summary_data.get('Moran\\'s I (Residuals)', 1)  # Default to high if not available\n",
        "    \n",
        "    if ground_truth_r2 > 0.7 and spatial_cv_r2 > 0.6:\n",
        "        print(\"  ‚úÖ EXCELLENT: Model shows strong predictive performance and good spatial generalization\")\n",
        "    elif ground_truth_r2 > 0.5 and spatial_cv_r2 > 0.4:\n",
        "        print(\"  ‚úÖ GOOD: Model shows satisfactory performance with reasonable spatial generalization\")\n",
        "    else:\n",
        "        print(\"  ‚ö†Ô∏è  MODERATE: Model performance could be improved, consider additional features or data\")\n",
        "    \n",
        "    if isinstance(moran_residuals, (int, float)) and abs(moran_residuals) < 0.2:\n",
        "        print(\"  ‚úÖ LOW SPATIAL AUTOCORRELATION: Model residuals show minimal spatial patterning\")\n",
        "    else:\n",
        "        print(\"  ‚ö†Ô∏è  SPATIAL PATTERNING DETECTED: Consider incorporating spatial features in future models\")\n",
        "    \n",
        "    # Recommendations\n",
        "    print(f\"\\nüéØ RECOMMENDATIONS:\")\n",
        "    \n",
        "    if ground_truth_r2 < 0.6:\n",
        "        print(\"  ‚Ä¢ Consider adding more relevant features or collecting additional training data\")\n",
        "    \n",
        "    if spatial_cv_r2 < ground_truth_r2 * 0.8:\n",
        "        print(\"  ‚Ä¢ Implement spatial cross-validation in future model development\")\n",
        "    \n",
        "    if isinstance(moran_residuals, (int, float)) and abs(moran_residuals) > 0.3:\n",
        "        print(\"  ‚Ä¢ Consider spatial regression techniques or adding spatial lag variables\")\n",
        "    \n",
        "    uncertainty = summary_data.get('Mean Uncertainty', 0)\n",
        "    if isinstance(uncertainty, (int, float)) and uncertainty > 0.3:\n",
        "        print(\"  ‚Ä¢ Prediction uncertainty is relatively high, consider ensemble methods\")\n",
        "    \n",
        "    print(\"\\nüöÄ NEXT STEPS:\")\n",
        "    print(\"  1. Deploy validated model for carbon sequestration assessment\")\n",
        "    print(\"  2. Monitor model performance with new data\")\n",
        "    print(\"  3. Consider model retraining with additional data\")\n",
        "    print(\"  4. Use uncertainty estimates in decision-making\")\n",
        "    \n",
        "    # Save comprehensive summary\n",
        "    summary_report = {\n",
        "        'validation_summary': summary_data,\n",
        "        'overall_assessment': {\n",
        "            'ground_truth_r2': ground_truth_r2,\n",
        "            'spatial_generalization': spatial_cv_r2,\n",
        "            'spatial_autocorrelation': moran_residuals,\n",
        "            'prediction_uncertainty': uncertainty\n",
        "        },\n",
        "        'validation_date': pd.Timestamp.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    with open('outputs/validation/comprehensive_validation_report.json', 'w') as f:\n",
        "        json.dump(summary_report, f, indent=2)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"‚úÖ COMPREHENSIVE VALIDATION COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nüìÅ Validation reports saved to outputs/validation/\")\n",
        "\n",
        "# Generate final summary\n",
        "generate_comprehensive_validation_summary()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
