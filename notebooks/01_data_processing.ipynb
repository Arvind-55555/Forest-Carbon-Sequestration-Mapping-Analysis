{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01_data_processing\n",
        "\n",
        "This notebook contains comprehensive data ingestion and preprocessing examples for biomass data analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Dependencies and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Enhanced Data Loader Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EnhancedBiomassLoader:\n",
        "    \"\"\"Enhanced loader for biomass data from multiple sources.\"\"\"\n",
        "    \n",
        "    def __init__(self, data_dir=\"data\"):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.data_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    def load_sample(self):\n",
        "        \"\"\"Returns a small list representing simplified pixel biomass values.\"\"\"\n",
        "        return [1.0, 2.5, 3.2, 0.8, 2.1]\n",
    "    \n",
        "    def create_sample_csv(self, filename=\"sample_biomass.csv\"):\n",
        "        \"\"\"Create a sample CSV file for demonstration.\"\"\"\n",
        "        data = {\n",
        "            'pixel_id': range(1, 101),\n",
        "            'biomass_value': np.random.normal(2.5, 1.0, 100).clip(0.1, 5.0),\n",
        "            'latitude': np.random.uniform(40.0, 45.0, 100),\n",
        "            'longitude': np.random.uniform(-75.0, -70.0, 100),\n",
        "            'vegetation_type': np.random.choice(['Forest', 'Grassland', 'Shrubland', 'Wetland'], 100)\n",
        "        }\n",
        "        df = pd.DataFrame(data)\n",
        "        filepath = self.data_dir / filename\n",
        "        df.to_csv(filepath, index=False)\n",
        "        return filepath\n",
    "    \n",
        "    def create_sample_json(self, filename=\"sample_biomass.json\"):\n",
        "        \"\"\"Create a sample JSON file for demonstration.\"\"\"\n",
        "        data = {\n",
        "            \"metadata\": {\n",
        "                \"dataset_name\": \"Biomass Sample Data\",\n",
        "                \"collection_date\": \"2024-01-15\",\n",
        "                \"units\": \"kg/mÂ²\"\n",
        "            },\n",
        "            \"biomass_readings\": [\n",
        "                {\n",
        "                    \"site_id\": f\"S{i:03d}\",\n",
        "                    \"biomass\": round(np.random.normal(3.0, 1.5), 2),\n",
        "                    \"coordinates\": {\n",
        "                        \"lat\": round(np.random.uniform(40.0, 45.0), 4),\n",
        "                        \"lon\": round(np.random.uniform(-75.0, -70.0), 4)\n",
        "                    },\n",
        "                    \"quality_flag\": np.random.choice([\"good\", \"fair\", \"poor\"], p=[0.8, 0.15, 0.05])\n",
        "                } for i in range(50)\n",
        "            ]\n",
        "        }\n",
        "        filepath = self.data_dir / filename\n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump(data, f, indent=2)\n",
        "        return filepath\n",
    "    \n",
        "    def load_csv(self, filename):\n",
        "        \"\"\"Load biomass data from CSV file.\"\"\"\n",
        "        filepath = self.data_dir / filename\n",
        "        if not filepath.exists():\n",
        "            raise FileNotFoundError(f\"CSV file not found: {filepath}\")\n",
        "        return pd.read_csv(filepath)\n",
    "    \n",
        "    def load_json(self, filename):\n",
        "        \"\"\"Load biomass data from JSON file.\"\"\"\n",
        "        filepath = self.data_dir / filename\n",
        "        if not filepath.exists():\n",
        "            raise FileNotFoundError(f\"JSON file not found: {filepath}\")\n",
        "        with open(filepath, 'r') as f:\n",
        "            return json.load(f)\n",
    "    \n",
        "    def load_multiple_formats(self, csv_file=None, json_file=None):\n",
        "        \"\"\"Load data from multiple file formats and combine.\"\"\"\n",
        "        datasets = {}\n",
        "        \n",
        "        if csv_file:\n",
        "            datasets['csv'] = self.load_csv(csv_file)\n",
        "        \n",
        "        if json_file:\n",
        "            json_data = self.load_json(json_file)\n",
        "            # Convert JSON to DataFrame for easier analysis\n",
        "            biomass_data = []\n",
        "            for reading in json_data['biomass_readings']:\n",
        "                biomass_data.append({\n",
        "                    'site_id': reading['site_id'],\n",
        "                    'biomass': reading['biomass'],\n",
        "                    'latitude': reading['coordinates']['lat'],\n",
        "                    'longitude': reading['coordinates']['lon'],\n",
        "                    'quality_flag': reading['quality_flag']\n",
        "                })\n",
        "            datasets['json'] = pd.DataFrame(biomass_data)\n",
        "        \n",
        "        return datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize Loader and Create Sample Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the enhanced loader\n",
        "loader = EnhancedBiomassLoader()\n",
        "\n",
        "# Create sample data files\n",
        "csv_file = loader.create_sample_csv()\n",
        "json_file = loader.create_sample_json()\n",
        "\n",
        "print(f\"Sample CSV created: {csv_file}\")\n",
        "print(f\"Sample JSON created: {json_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Loading and Basic Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data from different formats\n",
        "datasets = loader.load_multiple_formats(csv_file=\"sample_biomass.csv\", json_file=\"sample_biomass.json\")\n",
        "\n",
        "csv_data = datasets['csv']\n",
        "json_data = datasets['json']\n",
        "\n",
        "print(\"=== CSV Data Overview ===\")\n",
        "print(f\"Shape: {csv_data.shape}\")\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(csv_data.head())\n",
        "\n",
        "print(\"\\n=== JSON Data Overview ===\")\n",
        "print(f\"Shape: {json_data.shape}\")\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(json_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Comprehensive Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def explore_biomass_data(df, dataset_name):\n",
        "    \"\"\"Comprehensive exploration of biomass dataset.\"\"\"\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"EXPLORING {dataset_name.upper()} DATASET\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    # Basic information\n",
        "    print(f\"Dataset Shape: {df.shape}\")\n",
        "    print(f\"\\nData Types:\")\n",
        "    print(df.dtypes)\n",
        "    \n",
        "    # Statistical summary\n",
        "    print(f\"\\nStatistical Summary:\")\n",
        "    print(df.describe())\n",
        "    \n",
        "    # Check for missing values\n",
        "    print(f\"\\nMissing Values:\")\n",
        "    print(df.isnull().sum())\n",
        "    \n",
        "    # Biomass-specific analysis\n",
        "    biomass_col = 'biomass_value' if 'biomass_value' in df.columns else 'biomass'\n",
        "    if biomass_col in df.columns:\n",
        "        biomass_data = df[biomass_col]\n",
        "        print(f\"\\nBiomass-Specific Analysis:\")\n",
        "        print(f\"Sample size: {len(biomass_data)}\")\n",
        "        print(f\"Data range: {biomass_data.min():.2f} - {biomass_data.max():.2f}\")\n",
        "        print(f\"Mean biomass: {biomass_data.mean():.2f}\")\n",
        "        print(f\"Median biomass: {biomass_data.median():.2f}\")\n",
        "        print(f\"Standard deviation: {biomass_data.std():.2f}\")\n",
        "        print(f\"Coefficient of variation: {(biomass_data.std() / biomass_data.mean() * 100):.2f}%\")\n",
        "    \n",
        "    # Categorical data analysis (if exists)\n",
        "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "    for col in categorical_cols:\n",
        "        print(f\"\\n{col} value counts:\")\n",
        "        print(df[col].value_counts())\n",
        "\n",
        "# Explore both datasets\n",
        "explore_biomass_data(csv_data, \"CSV\")\n",
        "explore_biomass_data(json_data, \"JSON\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Data Visualization for Biomass Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_biomass_distribution(csv_df, json_df):\n",
        "    \"\"\"Create comprehensive visualizations for biomass data.\"\"\"\n",
        "    \n",
        "    # Set up the plotting figure\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    fig.suptitle('Biomass Data Distribution Analysis', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Extract biomass columns\n",
        "    csv_biomass = csv_df['biomass_value']\n",
        "    json_biomass = json_df['biomass']\n",
        "    \n",
        "    # 1. Histograms\n",
        "    axes[0, 0].hist(csv_biomass, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    axes[0, 0].set_title('CSV Data - Biomass Distribution')\n",
        "    axes[0, 0].set_xlabel('Biomass Value')\n",
        "    axes[0, 0].set_ylabel('Frequency')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    axes[0, 1].hist(json_biomass, bins=15, alpha=0.7, color='lightcoral', edgecolor='black')\n",
        "    axes[0, 1].set_title('JSON Data - Biomass Distribution')\n",
        "    axes[0, 1].set_xlabel('Biomass Value')\n",
        "    axes[0, 1].set_ylabel('Frequency')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Box plots\n",
        "    biomass_data = [csv_biomass, json_biomass]\n",
        "    axes[0, 2].boxplot(biomass_data, labels=['CSV', 'JSON'])\n",
        "    axes[0, 2].set_title('Biomass Distribution Comparison')\n",
        "    axes[0, 2].set_ylabel('Biomass Value')\n",
        "    axes[0, 2].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Violin plots\n",
        "    sns.violinplot(data=biomass_data, ax=axes[1, 0])\n",
        "    axes[1, 0].set_xticklabels(['CSV', 'JSON'])\n",
        "    axes[1, 0].set_title('Biomass Density Distribution')\n",
        "    axes[1, 0].set_ylabel('Biomass Value')\n",
        "    \n",
        "    # 4. Cumulative distribution function\n",
        "    sorted_csv = np.sort(csv_biomass)\n",
        "    sorted_json = np.sort(json_biomass)\n",
        "    \n",
        "    axes[1, 1].plot(sorted_csv, np.arange(len(sorted_csv)) / len(sorted_csv), \n",
        "                   label='CSV Data', linewidth=2)\n",
        "    axes[1, 1].plot(sorted_json, np.arange(len(sorted_json)) / len(sorted_json), \n",
        "                   label='JSON Data', linewidth=2)\n",
        "    axes[1, 1].set_title('Cumulative Distribution Function')\n",
        "    axes[1, 1].set_xlabel('Biomass Value')\n",
        "    axes[1, 1].set_ylabel('Cumulative Probability')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 5. Categorical analysis (for CSV data)\n",
        "    if 'vegetation_type' in csv_df.columns:\n",
        "        vegetation_means = csv_df.groupby('vegetation_type')['biomass_value'].mean()\n",
        "        axes[1, 2].bar(vegetation_means.index, vegetation_means.values, \n",
        "                     color=['lightgreen', 'lightblue', 'lightyellow', 'lightpink'])\n",
        "        axes[1, 2].set_title('Average Biomass by Vegetation Type')\n",
        "        axes[1, 2].set_xlabel('Vegetation Type')\n",
        "        axes[1, 2].set_ylabel('Average Biomass')\n",
        "        axes[1, 2].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Additional visualization: Quality flag analysis for JSON data\n",
        "    if 'quality_flag' in json_df.columns:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        quality_stats = json_df.groupby('quality_flag')['biomass'].agg(['mean', 'std', 'count'])\n",
        "        \n",
        "        plt.subplot(1, 2, 1)\n",
        "        json_df['quality_flag'].value_counts().plot(kind='pie', autopct='%1.1f%%', \n",
        "                                                 colors=['lightgreen', 'lightyellow', 'lightcoral'])\n",
        "        plt.title('Data Quality Distribution')\n",
        "        \n",
        "        plt.subplot(1, 2, 2)\n",
        "        sns.boxplot(data=json_df, x='quality_flag', y='biomass')\n",
        "        plt.title('Biomass Distribution by Quality Flag')\n",
        "        plt.xticks(rotation=45)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Generate visualizations\n",
        "visualize_biomass_distribution(csv_data, json_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Data Quality and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_biomass_data(df, biomass_column):\n",
        "    \"\"\"Perform data preprocessing and quality checks.\"\"\"\n",
        "    \n",
        "    print(\"=== DATA PREPROCESSING ===\")\n",
        "    \n",
        "    # Create a copy to avoid modifying original data\n",
        "    processed_df = df.copy()\n",
        "    \n",
        "    # 1. Handle missing values\n",
        "    missing_before = processed_df[biomass_column].isnull().sum()\n",
        "    if missing_before > 0:\n",
        "        print(f\"Found {missing_before} missing values in {biomass_column}\")\n",
        "        # Fill with median (you can choose other strategies)\n",
        "        processed_df[biomass_column].fillna(processed_df[biomass_column].median(), inplace=True)\n",
        "        print(f\"Missing values filled with median\")\n",
        "    \n",
        "    # 2. Remove outliers using IQR method\n",
        "    Q1 = processed_df[biomass_column].quantile(0.25)\n",
        "    Q3 = processed_df[biomass_column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    \n",
        "    outliers_before = len(processed_df)\n",
        "    processed_df = processed_df[(processed_df[biomass_column] >= lower_bound) & \n",
        "                              (processed_df[biomass_column] <= upper_bound)]\n",
        "    outliers_removed = outliers_before - len(processed_df)\n",
        "    \n",
        "    print(f\"Removed {outliers_removed} outliers using IQR method\")\n",
        "    print(f\"Data range after preprocessing: {processed_df[biomass_column].min():.2f} - {processed_df[biomass_column].max():.2f}\")\n",
        "    \n",
        "    # 3. Add derived features\n",
        "    processed_df['biomass_category'] = pd.cut(processed_df[biomass_column], \n",
        "                                            bins=[0, 1, 2, 3, 5], \n",
        "                                            labels=['Very Low', 'Low', 'Medium', 'High'])\n",
        "    \n",
        "    processed_df['log_biomass'] = np.log(processed_df[biomass_column] + 0.1)  # Add small constant to avoid log(0)\n",
        "    \n",
        "    print(f\"\\nAdded derived features:\")\n",
        "    print(f\"- biomass_category: Categorical classification\")\n",
        "    print(f\"- log_biomass: Log-transformed values for normalization\")\n",
        "    \n",
        "    return processed_df\n",
        "\n",
        "# Preprocess both datasets\n",
        "print(\"Processing CSV data:\")\n",
        "processed_csv = preprocess_biomass_data(csv_data, 'biomass_value')\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "print(\"Processing JSON data:\")\n",
        "processed_json = preprocess_biomass_data(json_data, 'biomass')\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PREPROCESSING COMPLETED SUCCESSFULLY!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary and Export Processed Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display final summary\n",
        "print(\"=== FINAL SUMMARY ===\")\n",
        "print(f\"Original CSV data shape: {csv_data.shape}\")\n",
        "print(f\"Processed CSV data shape: {processed_csv.shape}\")\n",
        "print(f\"Original JSON data shape: {json_data.shape}\")\n",
        "print(f\"Processed JSON data shape: {processed_json.shape}\")\n",
        "\n",
        "# Export processed data\n",
        "processed_csv.to_csv('data/processed_biomass_csv.csv', index=False)\n",
        "processed_json.to_csv('data/processed_biomass_json.csv', index=False)\n",
        "\n",
        "print(\"\\nProcessed data exported to:\")\n",
        "print(\"- data/processed_biomass_csv.csv\")\n",
        "print(\"- data/processed_biomass_json.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"DATA PROCESSING PIPELINE COMPLETED!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
